---
title: "ScPoEconometrics"
subtitle: "Session 3"
author: "Florian Oswald"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)

```

# Data On Cars

.pull-left[
* Suppose we had data on car `speed` and stopping `dist` (ance):

```{r}
head(cars)
```

* How are `speed` and `dist` related?

* What is a good summary of their relationship?
]

--

.pull-right[
```{r,echo=FALSE,fig.align='center',fig.height=7,fig.width=8}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
```
]

---

# A Line throught the Scatterplot of Cars

.left-wide[
```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=7}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 60,b = 0,lw=3)
```
]

--

.right-thin[
<br>
<br>

* A *line*! Great. But **which** line? This one?

* That's a *flat* line. But `dist` is increasing. 

* `r emo::ji("weary")`

]

---

# A Line throught the Scatterplot of Cars

.left-wide[
```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=7}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 0,b = 5,lw=3)
```
]

--

.right-thin[
<br>
<br>

* **That** one?

* Slightly better. Has a *slope* and an *intercept*.

* `r emo::ji("neutral_face")`

]


---

# Writing Down A *Line*

.pull-left[
* We observe $(y_i,x_i)$ in the data.

* This describes a line with intercept $b_0$ and slope $b_1$:
    $$
    \widehat{y}_i = b\_0 + b\_1 x\_i
    $$

* We call $\widehat{y}_i$ the *prediction* for $y_i$.

* Most of the times, $\widehat{y}_i \neq y_i$, i.e. we make an *error*.
]

--

.pull-right[

* At point $x_i$ we make error $e_i$.

* Our aim will be to keep the error *as small as possible*, while at the same time giving a reasonable description of the data.

* (We could be more generally trying to fit a *curve* rather than a *line*, by the way.)

* The *actual data* $(y_i,x_i)$ can thus be written like *prediction + error*:
    $$
    y_i = b_0 + b_1 x_i + e_i
    $$
]



---

# Making Errors

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "black")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 4,col = "black")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

.left-wide[
```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height=5.5,fig.width=8}
par(mar = lowtop)
plot_unexp_dev(plot_data)
par(mar = om)
```
]

.right-thin[
* Red Arrows are *errors* or *residuals* for each prediction.

* Often denoted $u$ or $e$.

* Note that we have both $e>0$ and $e<0$!
]

---
class: inverse

# App Time!

* Let's try to find the best line using only the *absolute value* of errors!

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows') # explainer about app
```

---

# Writing Down *The Best* Line

.pull-left[
* choose $(b_0,b_1)$ s.t. the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**

* $e_1^2 + \dots + e_N^2$ is the *sum of squared residuals*, or SSR.

* Wait a moment... Why *squared* residuals?!
]

--

.pull-right[

* In previous plot, errors of different sign $(+/-)$ cancel out!

* This makes it hard to find a good line.

* Squaring each $e_i$ solves that issue as $e_i^2 \geq 0, \forall i$.
]


---

# Best Line and Squared Errors

.left-wide[
```{r line-squares, echo=FALSE, message=FALSE, warning=FALSE,fig.width=8,fig.height = 5.5}
par(mar = lowtop)
plot_unexp_SSR(plot_data)
par(mar = om)
```
]

--

.right-thin[
<br>
<br>

* **That's**  the one!

* Perfect! Minimizes the sum of squares.

* `r emo::ji("relieved")`

]

---
class: inverse

# App Time!


```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```


---

# Ordinary Least Squares (OLS)

.pull-left[
* OLS estimates the best line for us.

* In our single regressor case, there is a simple formula for the slope:
  $$
  b_1 = \frac{cov(x,y)}{var(x)}
  $$
  
* and for the intercept
  $$
  b_0 = \bar{y} - b_1\bar{x}
  $$
]

--

.pull-right[

* `r emo::ji("rotating_light")`

* You **must** know those formulae!

]
  
  
---
class: inverse

# App Time!

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```


---
class: inverse

# App Time!


Let's do some more OLS!

```{r,eval=FALSE}
launchApp('reg_full')
aboutApp('reg_full')  # after
```

---

# OLS without any Regressor

.pull-left[
* Our line is flat at level $b_0$:
  $$y = b_0$$
* Our optimization problem is now
  $$b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,$$
* With solution
  $$b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.$$
]

--

.pull-right[
* In other words: Estimates the **mean** of $y$!

```{r,echo = FALSE, fig.height = 6}
par(mar = lowtop)
plot_total_dev(plot_data)
par(mar = om)
```
]


---

# Other OLS Restrictions

* There are other restrictions we can impose.

* They are described [in the book](https://scpoecon.github.io/ScPoEconometrics/linreg.html#OLS). Optional `r emo::ji("nerd_face")`.

* There is an app for each of them:
<br>
<br>

type | App  
-------- | --------
No Intercept | `launchApp('reg_constrained')` 
Centered Regression | `launchApp('demeaned_reg')` 
Standardized Regression | `launchApp('reg_standardized')`


---

# Predictions and Residuals


1. The error is $e_i = y_i - \widehat{y}_i$

2. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$\begin{align}\frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$
--
3. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.

---

# Properties of Residuals

.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Let's look at the data behind our *arrows* plot above:
]

--

.pull-right[
```{r,echo=FALSE}
library(huxtable)
sd = subset(plot_data,select=c(x,y,y_hat,error))
sd = rbind(sd, colMeans(sd))
sd = cbind(c(rep("",nrow(sd)-1),"Means"),sd)
names(sd)[1] = ""
ss = hux(sd)
ss %>%
  add_colnames() %>%
  set_number_format(2) %>%
  set_bottom_border(c(1,nrow(sd)), 2:5,2) %>%
  set_align(row = 1,everywhere, "center")
```

]

---

# Properties of Residuals


.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

]

--

.pull-right[

```{r}
# 1.
all.equal(mean(sd$y_hat), mean(sd$y))

# 2.
all.equal(mean(sd$error), 0)

# 3.
all.equal(cov(sd$error,sd$y_hat), 0)
```
]


---

# Linear Statistics

* It's important to keep in mind that Var, Cov, Corr and Regression measure **linear relationships** between two variables.

* Two datasets with *identical* correlations could look *vastly* different.

* They would have the same regression line.

* Same correlation coefficient.

--

* Is that even possible?

---

# Linear Statistics: Anscombe

* Francis Anscombe (1973) comes up with 4 datasets with identical stats. But look!

.left-wide[

```{r,echo=FALSE,fig.height = 5}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "blue")
}
par(op)
```
]

--

.right-thin[
```{r,echo = FALSE}
ch = hux(covs)
ch %>% 
  set_number_format(row = everywhere, col = c(2:4), 2) %>%
  set_number_format(row = everywhere, col = 1, 0) %>%
  add_colnames()

```

]
---

# Dinosaurs in your Data?

* So, be wary of only looking a linear summary stats.
* Also look at plots.
* Dinosaurs?
    ```{r,eval=FALSE}
    launchApp("datasaurus")
    aboutApp("datasaurus")
    ```

---

# Nonlinear Relationships in Data?

* We can accomodate non-linear relationships in regressions.

* We'd just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
    
* This is *multiple regression* (next chapter!)

---

# Nonlinear Relationships in Data?

* For example, suppose we had this data and fit the above regression:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 5}
    l1 = lm(mpg~hp+I(hp^2),data=mtcars)
    newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
    newdata$y = predict(l1,newdata=newdata)
    plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y",pch = 20, cex = 2)
    grid()
    lines(newdata$hp,newdata$y,lw=3,col = "red")
    ```

---
# Analysis of Variance

* Remember that $y_i = \widehat{y}_i + e_i$.

* We have the following decomposition:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    
* Because: $Cov(\hat{y},e)=0$

* Total variation (SST) = Model explained (SSE) + Unexplained (SSR)


---

# Assessing the Goodness of Fit

* The $R^2$ measures how good the model fits the data.

* $R^2=1$ is very good, $R^2=0$ is very poorly.
    $$
    R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
    $$
    
* Small $R^2$ doesn't mean it's a useless model!


---

# An Example - California Test Scores

* Let's look at [the worked example](https://scpoecon.github.io/ScPoEconometrics/linreg.html#lm-example1) in the book!

---

# Rescaling Regressions

* Suppose outcome $y$ is *income in Euros*

* $x$ be years of schooling
  $$
  y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
  $$

* Assume that $\beta_1 = 2000$, s.t. each additional year of schooling gives 2000 Euros more income.

* What is $\beta_1$ if we measure $y$ in *thousands of euros* instead?


---
# Rescaling Regressions


* The result depends on whether we rescale $y$, $x$ or both.

* If we have rescale $x$ by $c$ and $y$ by $d$, one can show that
    $$\begin{align} b_1' &= \frac{Cov(dx,cy)}{Var(dx)} \\ &= \frac{d}{c} b \end{align}$$
    
* and the intercept
    $$\begin{align} b_0' &= \mathbb{E}[d\cdot Y] - \frac{Cov(cX, dY)}{Var(cX)} \mathbb{E}[c\cdot X] \\ &= d b_0\end{align}$$

---

# App: Rescaling Regressors


```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp('Rescale')
```


```{r,eval=FALSE}
library(ScPoEconometrics)
runTutorial('rescaling')
```


---


class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

