---
title: "ScPoEconometrics"
subtitle: "Simple Linear Regression"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

library("magrittr")
library("repmis")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("haven")
library("ggpubr")
library("huxtable")
```

# Recap from past weeks

* `R` basics, importing data

* Exploratory data analysis:

    * Summary statistics: *mean*, *median*, *variance*, *standard deviation*
    * Data wrangling: `dplyr`
    * Data visualization: base `R` and `ggplot2`

--

## Today - Real metrics' finally `r emo::ji("v")`

* Introduction to the __Simple Linear Regression Model__ and __Ordinary Least Squares__ *estimation*.

* Empirical applications:

  - *Class size* and *student performance*,
  - **Education** and **wages** (si on ne fait pas le log cet exemple ne sert à rien je trouve + je doute qu'on ait le temps de le faire)

---

# Class size and student performance

* What policies lead to improved student learning?

* Class size reduction has been at the heart of policy debates for *decades*.

--

* We will be using data from a famous paper by [Joshua Angrist and Victor Lavy (1999)](https://economics.mit.edu/files/8273)

* Consists of test scores and class/school characteristics for fifth graders (10-11 years old) in Jewish public elementary schools in Israel in 1991.

* National tests measured *mathematics* and (Hebrew) *reading* skills.

---

class:: inverse

# Task 1: Getting to know the data (7 minutes)
 
1. Load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1).  
You need to find the function that enables importing *.dta* files. (FYI: *.dta* is the extension for data files used in [*Stata*](https://www.stata.com/))

1. Describe the dataset:

  * What is the unit of observations, i.e. what does each row correspond to?
  * How many obseravtions are there?
  * What variables do we have? View the dataset to see what the variables correspond to.
  * What do the variables `avgmath` and `avgverb` correspond to?

1. Do you have any priors about the actual (linear) relationship between class size and student achievement? What would you do to get a first insight?

```{r, echo=FALSE}
# import data
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")
```

---

# Class size and student performance: Scatter plot

.pull-left[
```{r, echo=FALSE,fig.height=6}
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics") +
    theme_bw(base_size = 20)
g_math
```
]

.pull-right[
```{r, echo=FALSE,fig.height=6}
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Class size",
         y = "Average score",
         title = "Reading") +
    theme_bw(base_size = 20)
g_verb
```
]

--

* Hard to see much because all the data points are aligned vertically. Let's add a bit of `jitter` to disperse the data slightly.

---

# Class size and student performance: `jitter` scatter plot 

.pull-left[
```{r, echo=FALSE,fig.height=6}
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.5, position = "jitter") +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics") +
    theme_bw(base_size = 20)
g_math
```
]

.pull-right[
```{r, echo=FALSE,fig.height=6}
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 2, alpha = 0.5, position = "jitter") +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Class size",
         y = "Average score",
         title = "Reading") +
    theme_bw(base_size = 20)
g_verb
```
]

--

* Seems to be a positive association but not easy to tell. Let's compute the average score by class size!

---

class:: inverse

# Class size and student performance: Binned scatter plot <br/> (7 minutes) 

1. Create a new dataset (`grades_avg_cs`) where math and verbal scores are averaged by class size. Let's call these new average scores `avgmath_cs` and `avgverb_cs`.  
*N.B.: the "raw" scores are already averages at the class level. Here we average these averages by class size.*

1. Redo the same plots as before. Is the sign of the relationship more apparent?

```{r, echo = FALSE}
grades_avg_cs = grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))
```

---

# Class size and student performance: Binned scatter plot 

.pull-left[
```{r, echo=FALSE,fig.height=6}
g_math_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
    geom_point(size = 2) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics") +
    theme_bw(base_size = 20)
g_math_cs
```
]

.pull-right[
```{r, echo=FALSE,fig.height=6}
g_verb_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgverb_cs)) + 
    geom_point(size = 2) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Class size",
         y = "Average score",
         title = "Reading") +
    theme_bw(base_size = 20)
g_verb_cs
```
]

---

# Class size and student performance: Binned scatter plot

* We'll first focus on the mathematics scores and for visual simplicity we'll zoom in

```{r, echo=FALSE,fig.height=4.75, fig.width = 8}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14)
```

---

# Class size and student performance: Which line?

How to visually summarize the relationship: **a line through the scatter plot**

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  geom_hline(yintercept = 65, col = "red")
```
]

--

.right-thin[
<br>

* A *line*! Great. But **which** line? This one?

* That's a *flat* line. But average mathematics score is somewhat *increasing* with class size `r emo::ji("weary")`

]

---

# Class size and student performance: Which line?

How to visually summarize the relationship: **a line through the scatter plot**

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
g_math_cs +
  ylim(50, 80) +
  theme_bw(base_size = 14) +
  geom_abline(intercept = 55,slope = 0.6, col = "red")
```
]

--

.right-thin[
<br>

* **That** one?

* Slightly better! Has a **slope** and an **intercept** `r emo::ji("neutral_face")`

* We need a rule to decide! 

]


---

# Writing Down A *Line*

Let's formalise a bit what we are doing so far. 

* We are interested in the relationship between two variables:
  * an __outcome variable__ (also called __dependent variable__):  
  *average mathematics score* $(y_i)$
  
  * an __explanatory variable__ (also called __independent variable__ or __regressor__):  
  *class size* $(x_i)$
  
--

* For each class we observe both $x_i$ and $y_i$, and therefore we can plot the *joint distribution* of class size and average mathematics score.

--

* We summarise this relationship with a line (for now). The equation for such a line with an intercept $b_0$ and a slope $b_1$ is:
    $$
    \widehat{y}_i = b\_0 + b\_1 x\_i
    $$

--

* $\widehat{y}_i$ is our *prediction* for $y_i$ given our model (i.e. the line).

---

# Writing Down A *Line* : Error term

* If all the data points were __on__ the line then $\widehat{y}_i = y_i$.

--

```{r, echo = FALSE, fig.height = 4,fig.width=7}
x <- runif(50, min  = 0, max = 1)
y <- 1 * x

data <- data.frame(y = y,
                   x = x)

plot_ex <- data %>% ggplot(aes(x = x, y = y)) +
  geom_point() +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(x = "x",
       y = "y") +
  theme_bw(base_size = 14)
plot_ex
```

---

# Writing Down A *Line* : Error term

* If all the data points were __on__ the line then $\widehat{y}_i = y_i$.

```{r, echo = FALSE, fig.height = 4,fig.width=7}
plot_ex + geom_line(color = "red")
```

---

# Writing Down A *Line* : Error term

* If all the data points were __on__ the line then $\widehat{y}_i = y_i$.

* However, since student performance is not *only* explained by class size, $\widehat{y}_i \neq y_i$, i.e. we make an __error__. This __error__ is called the __error term__.

--

* At point $(x_i,y_i)$, we note this error $e_i$.

--

* The *actual data* $(y_i,x_i)$ can thus be written as *prediction + error*:

  $$
  \overbrace{y_i}^\text{actual outcome} = \underbrace{b_0 + b_1 x_i}_\text{prediction} + \underbrace{e_i}_\text{error term}
  $$

--

* **Goals**
  * Find the values for $b_0$ and $b_1$ that **make the errors as small as possible**,
  * Check whether these values **give a reasonable description of the data**.


---

# The Error Term: Visually

Have x-axis go to 0. + make this graph nicer!
ideally with the class size and student performance data no?

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "black")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 4,col = "black")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

.left-wide[
```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height=5.5,fig.width=8}
par(mar = lowtop)
plot_unexp_dev(plot_data)
par(mar = om)
```
]

.right-thin[

</br>
</br>

* Red Arrows are *errors* or *residuals* for each prediction.

* *Errors* can be either positive (bottom to top arrows) or negative.

]

---
class: inverse

# App Time!

Intuitively one might want to simply minimize the sum of all the errors
$\sum^n_{i=1}{e_i}$.

Let's try to find the best line by minimizing the sum of the errors.

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows') # explainer about app
```

---

# Writing Down A *Line*


.pull-left[

</br>

* Errors of different sign $(+/-)$ cancel out, so let's consider **squared residuals** 
$$\forall i \in [1,N], e_i^2 = (y_i - b_0 - b_1 x_i)^2$$

* Choose $(b_0,b_1)$ such that the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**.

* ***Sum of Squared Residuals*** (SSR): $e_1^2 + \dots + e_N^2$
]

--

.pull-right[

</br>

```{r, echo=FALSE, message=FALSE, warning=FALSE,fig.width=6,fig.height = 4.5}
par(mar = lowtop)
plot_unexp_SSR(plot_data)
par(mar = om)
```
]

---

class: inverse

# App Time!

Let's minimize some squared errors! 
```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```

---

# **O**rdinary **L**east **S**quares (OLS): Coefficient Formulas

* **OLS**: estimation technique consisting in minimizing the sum of squared residuals.

* So what are the formulae for $b_0$ (intercept) and $b_1$ (slope)?

--

* In our single independent variable case:

   ### __Slope: $b_1^{OLS} = \frac{cov(x,y)}{var(x)}$ $\hspace{2cm}$ Intercept: $b_0^{OLS} = \bar{y} - b_1\bar{x}$__

--

* `r emo::ji("exclamation")` You should know these formulas, especially the one for $b_1^{OLS}$ `r emo::ji("exclamation")`
   
--

* These formulas do not appear from magic. They can be found by solving the minimisation of squared errors. The maths can be found [here](https://www.youtube.com/watch?v=Hi5EJnBHFB4) for those who are interested.

---

class: inverse

# App Time!

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```

---

# **O**rdinary **L**east **S**quares (OLS): Interpretation

For now assume both the dependent variable $(y)$ and the independent variable $(x)$ are numeric.

--

* Intercept $(b_0)$: **The predicted value of $y$ $(\widehat{y})$ if $x = 0$.**

--

* Slope $(b_1)$: **The change in the predicted value of $y$ associated to a one-unit increase in $x$.**

--

## Graphical Interpretation

Create graph

---

# Application: Class size and student performance

Let's go back our class size and student performance data. Let's display the OLS line.

.left-wide[

```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
g_math_cs +
  ylim(50, 80) +
  theme_bw(base_size = 14) +
  geom_smooth(method = "lm", se = F, col = "darkgreen")
```
]

--

.right-thin[

</br>

* Some points (observations) are poorly fitted by the OLS line. 

* We call them **outliers**.
]

---

# OLS with `R`

* In `R`, OLS regressions are estimated using the `lm` function.

* This is how it works:

  ```{r, echo = TRUE, eval = FALSE}
  lm(dependent variable ~  independent variable, data = data.frame containing the data)
  ```

--

## Class size and student performance

Let's estimate the following model by OLS: $\textrm{avgmath_cs}_i = b_0 + b_1 \textrm{classsize}_i + e_i$

```{r echo=T, eval = TRUE}
# OLS regression of class size on average maths score
lm(formula = avgmath_cs~classize, data = grades_avg_cs) 
```

---

class: inverse

# Task 3: OLS Regression (7 minutes)

1. Run the same OLS regression by yourself but using average verbal score as the dependent variable.

2. Is the slope coefficient similar to the one found for average math score? Was it expected based on the graphical evidence?

3. What is the predicted average verbal score when class size is equal to 0? (Does that even make sense?!)

3. What is the predicted average verbal score when the class size is equal to 30 students?


---

# Standardized regression 

Standardizing a variable $z$:
$$ \widetilde z = \frac{z - \bar z}{sd(z)} $$
where $\bar z$ is $z$'s *mean* and $sd(z)$ is $z$'s *standard deviation*.

Why would we standardize the variables in the regression?

--

* To be able to __interpret the *magnitude* of an independent variable's coefficient__. Remember that the OLS coefficient is sensitive to the independent variable's unit. Why?

--

* The dependent variable or independent variable's __units are difficult to interpret__ in themselves.  
*Example:* Is a 1-point increase in student test scores meaningful? Depending on the spread of the scores, not clear whether that would be a big or small change...

--

* To __combine variables together__.  
*Example:* if we want to combine the mathematics and verbal scores together.

---

class: inverse

# Task 4: Standardized OLS

1. Create 2 variables (`std_math` and `std_verb`) that contain the standardized average math and verbal scores.
 
2. Create a new variable `std_score` that associates to each class size a unique score.

3. Create a new variable `std_classize` that is the standardized class size.

4. Run the OLS regression of `std_score` on `std_classize`. 

5. Interpret the value of each coefficient.

---

# Other OLS variations / restrictions

* All are described [in the book](https://scpoecon.github.io/ScPoEconometrics/linreg.html#OLS). Optional `r emo::ji("nerd_face")`.

* There is an app for each of them:
<br>
<br>

type | App  
-------- | --------
No Intercept, No regressors | `launchApp('reg_constrained')` 
Centered Regression | `launchApp('demeaned_reg')` 
Standardized Regression | `launchApp('reg_standardized')`


---

# Predictions and Residuals => maybe show here that prediction is uncorrelated with error


1. The error term is $e_i = y_i - \widehat{y}_i$

2. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$\begin{align}\frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$
--

3. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.

---


# Properties of Residuals => je ne comprends vraiment pas l'intérêt de ce slide. les données du arrow plot ne valent rien. autant leur montrer avec la regression de class size sur average math score, non!

.pull-left[
1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Let's look at the data behind our *arrows* plot above:

]

--

.pull-right[

```{r,echo=FALSE, results="asis"}
sd = subset(plot_data,select=c(x,y,y_hat,error))
sd = rbind(sd, colMeans(sd))
sd = cbind(c(rep("",nrow(sd)-1),"Means"),sd)
names(sd)[1] = ""

ss = hux(sd,add_colnames = T) %>%
  set_number_format(2) %>%
  set_bottom_border(c(1,nrow(sd)), 1:5,2) %>%
  set_align(row = 1,everywhere, "center")
ss
```

]

---

# Properties of Residuals => même commentaire que ci-dessus

.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).
]

--

.pull-right[
```{r}
# 1.
all.equal(mean(sd$y_hat), mean(sd$y))

# 2.
all.equal(mean(sd$error), 0)

# 3.
all.equal(cov(sd$error,sd$y_hat), 0)
```
]

---


# Linearity

* It's important to keep in mind that covariance, correlation and simple OLS regression only measure **linear relationships** between two variables.

* Two datasets with *identical* correlations and regression lines could look *vastly* different.

--

* Is that even possible?
<img src="https://media.giphy.com/media/5aLrlDiJPMPFS/giphy.gif" height = "400" align = "middle" />


---

# Linearity: Anscombe

* Francis Anscombe (1973) came up with 4 datasets with identical stats. But look!

.left-wide[

```{r,echo=FALSE,fig.height = 4}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "green")
}
par(op)
```
]

--

.right-thin[
```{r,echo = FALSE}
ch = hux(covs)
ch %>%
  set_number_format(row = everywhere, col = c(2:4), 2) %>%
  set_number_format(row = everywhere, col = 1, 0) %>%
  add_colnames()
```

]
---

# Nonlinear Relationships in Data?

.pull-left[
* We can accomodate non-linear relationships in regressions.

* Just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
    
* This is *multiple regression* (in 2 weeks!)
]

--

.pull-right[
* For example, suppose we had this data and fit the previous regression model:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 6}
    data(mtcars)
    mtcars %>% ggplot(aes(x = hp, y = mpg)) +
      geom_point() +
      stat_smooth(method='lm', formula = y~poly(x,2), se = FALSE) +
      labs(x = "x",
           y = "y",
           title = "Nonlinear relationship between x and y") +
    theme_bw(base_size = 20)

    ```
]

---

# Analysis of Variance

* Remember that $y_i = \widehat{y}_i + e_i$.

* We have the following decomposition:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    
* Because:
  * $Var(x+y) = Var(x) + Var(y) + 2Cov(x,y)$
  * $Cov(\hat{y},e)=0$

* __Total variation (SST) = Model explained (SSE) + Unexplained (SSR)__


---

# Goodness of Fit

* The $R^2$ measures how well the model fits the data.

--

$$
R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
$$

--
    
* $R^2$ close to $1$ indicates a very high explanatory power of the model

* $R^2$ close to $0$ means that the variations in the outcome $(y)$ are very poorly captured.

--

* Interpretation: an $R^2$ of 0.5, for example, means that the variation in $x$ "explains" 50% of the variation in $y$.

--
    
* `r emo::ji("warning")` Low $R^2$ does __NOT__ mean it's a useless model! Remember that econometrics is interested in causal mechanisms, not prediction!

---


class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# SEE YOU NEXT WEEK!


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

---

# Education and wage

* Let's now consider the relationship between between education (years of schooling) and (hourly) wage.

* The following data were collected in the [1976 Current Population Survey](https://www.census.gov/programs-surveys/cps/library.1976.html) in the US. 


* Load it from the `wooldridge` package with the `data()` function.

```{r echo = FALSE}
data("wage1", package = "wooldridge")   # load data

# a function that returns a plot
plotfun <- function(wage1,log=FALSE,rug = TRUE){
    y = wage1$wage
    if (log){
        y = log(wage1$wage)
    }
    plot(y = y,
       x = wage1$educ, 
       col = "red", pch = 21, bg = "grey",     
       cex=1.25, xaxt="n", frame = FALSE,      # set default x-axis to none
       main = ifelse(log,"log(Wages) vs. Education, 1976","Wages vs. Education, 1976"),
       xlab = "years of education", 
       ylab = ifelse(log,"Log Hourly wages","Hourly wages"))
    axis(side = 1, at = c(0,6,12,18))         # add custom ticks to x axis
    if (rug) rug(wage1$wage, side=2, col="red")        # add `rug` to y axis
}
```

* Briefly describe the data and produce a plot to vizualize the link between `wage` and `education`.

--

```{r echo = FALSE, fig.height = 3, fig.width=10}
par(mfcol = c(1,2))  # set up a plot with 2 panels
# plot 1: standard scatter plot
plotfun(wage1)

# plot 2: add a panel with histogram+density
hist(wage1$wage,prob = TRUE, col = "grey", border = "red", 
     main = "Histogram of wages and Density",xlab = "hourly wage")
lines(density(wage1$wage), col = "black", lw = 2)
```
---

class: inverse

# Education and wage

 **TASK 5**

* Run the regression of the hourly wage on education. 

* Interpretation

    * With zero year of education, the hourly wage is about **??** dollars per hour.

    * Each additional year of education increase hourly wage by **??** cents.

    * Compute the predicted wage associated to 20 years of education.
    
---

# Education and wage : log transformation

You may have noticed that the relationship between wages and education is not perfectly linear. 

.pull-left[
```{r, echo = F, fig.height=5}
plotfun(wage1, log = T, rug = F)
```
]

.pull-right[

TBD
]
