---
title: "ScPoEconometrics"
subtitle: "Confidence Intervals and Hypothesis Testing"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

library(magrittr)
library(moderndive)
library(dplyr)
library(tidyverse)
library(infer)

overwrite = FALSE

```

# Recap from last week

* Sampling: ***sampling variation*** and ***distribution distributions***

* population, population parameter, sample statistic or point estimate, random sampling

* ***Unbiased estimator***: $\mathop{\mathbb{E}}[\hat{p}] = p$

* ***Central Limit Theorem***: as sample increases, sampling distribution is more ***normally distributed***, centered around the ***population parameter***, and has a smaller ***standard error***

--

## Today: Deeper dive into ***statistical inference*** <sup>1</sup>

.footnote[
[1]: This lecture is based on the wonderful [confidence interval](https://moderndive.com/8-confidence-intervals.html) and [hypothesis testing](https://moderndive.com/9-hypothesis-testing.html) chapters of [ModernDive](https://moderndive.com/)]

* Process of making claims about a population based on information from a sample

* *Confidence intervals*: providing plausible ***range*** of values

* *Hypothesis testing*: comparing statistics between groups

---

background-image: url(https://media.giphy.com/media/hXG93399r19vi/giphy.gif)
background-position: 18% 45%
background-size: 450px

# Back to reality (there goes gravity `r emo::ji("wink")`)

.pull-right[

* In real life we only get to take ***one*** sample from the population (not ***1000***!).

* Also, we obviously don't know the true population parameter, that's what we are interested in!

* So what on earth was all of this good for? Fun Only?! `r emo::ji("anguished")`

]

--

<br>
<br>

* Even unobserved, we **know** that the sampling distribution does exist, and even better, we know how it behaves!

* Let's see what we can do with this...

---

layout: false
class: title-slide-section-red, middle

# Confidence Intervals

---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# From Point Estimates to Confidence Intervals

* Until now, we have only estimated ***point estimates*** from our samples: *sample means*, *sample proportions*, *regression coefficients*, etc.

--

* We know that this ***sample statistic*** differs from the ***true population parameter*** due to ***sampling variation***. 

--

* Rather than a point estimate, we could give a ***range of plausible values*** for the population parameter.

--

* That is precisely what a ***confidence interval*** (CI) provides.

---

# Constructing Confidence Intervals 

There are several approaches to constructing confidence intervals:
    
  1. *Theory*: Use mathematical formulas (***Central Limit Theorem***) to derive the sampling distributions of our point estimates under certain conditions.

--

  1. *Simulation.* We can use the ***bootstrapping*** method to *reconstruct* the sampling distribution.
    
--
    
* We'll focus on simulation to give you the intuition and come back to the maths approach next week.

--

* But first: what is a *bootstrap*?

---

# Bootstrapping

* ***Key idea***: *pretend* the sample **is** the population, and repeatedly *resample with replacement* from it until obtaining a new sample of the same size as the original sample.

* Bootstrapping does ***not*** improve point estimation.

* But it yields a very good estimate of the ***standard error*** of the ***sampling distribution***.

---

background-image: url(https://media.giphy.com/media/7pHTiZYbAoq40/giphy.gif)
background-position: 50% 50%
background-size: 800px

---

# Back to Pasta

```{r, echo = FALSE}
bowl <- read.csv("https://www.dropbox.com/s/qpjsk0rfgc0gx80/pasta.csv?dl=1")

p = mean(bowl$color=="green")
sample_size = 40

set.seed(5)
my_sample = bowl %>%
  mutate(color = as.factor(ifelse(color == "green","green","non-green"))) %>%
  rep_sample_n(size = sample_size) %>%
  ungroup() %>%
  select(pasta_ID, color) %>%
  arrange(pasta_ID)

p_hat = mean(my_sample$color=="green")
```

* As in real life, we have access to *only one random sample* from the entire population.

--

* So let's start by drawing one random sample of size $n = 40$ from our bowl:

```{r, eval = FALSE}
bowl <- read.csv("https://www.dropbox.com/s/qpjsk0rfgc0gx80/pasta.csv?dl=1")

my_sample = bowl %>%
  mutate(color = as.factor(ifelse(color == "green","green","non-green"))) %>%
  rep_sample_n(size = 40) %>%
  ungroup() %>%
  select(pasta_ID, color) %>%
  arrange(pasta_ID)
```

.pull-left[
```{r}
head(my_sample)
```
]

.pull-right[
```{r}
p_hat = mean(my_sample$color == "green")
```

The proportion of green pasta in this sample is: $\hat{p} = `r p_hat`$.

]
---

# Boostrapping our pasta sample

How do we get 1 **bootstrap sample**:
  
  1. Randomly pick one pasta from your sample and record the associated color.
  
  1. Put this pasta back in your sample.
  
  1. Repeat step 1 to 2 `r sample_size-1` times (i.e. $n-1$)

  1. Compute the proportion of green pasta from your bootstrap sample

---

# Boostrapping our pasta sample

Here is one bootstrap sample: 

.pull-left[

```{r,eval=FALSE}
one_bootstrapp = my_sample %>%
  rep_sample_n(size = 40, replace = TRUE) %>%
  arrange(pasta_ID)
```

```{r,echo=FALSE}
set.seed(5)
one_bootstrapp = my_sample %>%
  rep_sample_n(size = 40, replace = TRUE) %>%
  arrange(pasta_ID)
one_bootstrapp
```
]

--

.pull-right[

* All pasta have been draw from our initial sample

* You can notice that:

  * Several pasta have been drawn multiple times (ID=128,149,...)! That's normal.

  * The bootstrap sample size is equal to our initial sample size. 

]

  
---

# Getting the Bootstrap distribution

* Repeating the whole procedure 1000 times, you will get 1000 bootstrap samples and 1000 bootstrapped estimates! 

* We use the `infer` package that makes the whole bootstrapping procedure easier. 

```{r, echo=FALSE}
bootstrap_distrib = my_sample %>% # take my random sample
  specify(response = color, success = "green") %>% 
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop")

percentile_95ci = get_confidence_interval(bootstrap_distrib, level = 0.95, type = "percentile")
percentile_95lb = percentile_95ci[[1]]
percentile_95ub = percentile_95ci[[2]]

```


```{r, eval = FALSE}
bootstrap_distrib = my_sample %>% # take my random sample
  specify(response = color, success = "green") %>% # specify the variable and level of interest
  generate(reps = 1000, type = "bootstrap") %>% # generate 1000 bootstrap samples 
  calculate(stat = "prop") # calculate the proportion of green pasta for each
```

--

.pull-left[

* Here is the first 3 rows:

```{r}
head(bootstrap_distrib,3)
```
]

--

.pull-right[

</br>

***Question***

* What `r bootstrap_distrib$stat[2]` is referring to? 

* How many rows has this dataframe? 
]


---

# Visualizing Our Simulated Distribution

```{r,echo = FALSE,fig.height=5}
visualize(bootstrap_distrib,fill = "darkgreen") + 
  scale_x_continuous(limits = c(0.3, 0.8)) +
  # geom_vline(xintercept = p_hat, col = "green", size = 1, linetype = 2) +
      labs(x = "Proportion of green pasta",
       y = "Frequency",
       title = "Simulated Bootstrap Distribution") +
  theme_bw(base_size = 14)
```

---

# Building a 95% confidence interval

* Let's define an interval that covers 95% of our bootstrapped estimates.

* We get it by cutting the boostrap distribution at the 2.5% and 97.5% quantiles

* It defines our 95% CI as: $CI^B = [q_{2,5\%} ; q_{97,5\%}]$

.pull-left[

</br>

* In our case gives: $[`r round(percentile_95lb,2)` ; `r  round(percentile_95ub,2)`]$

```{r}
c(quantile(bootstrap_distrib$stat,0.025),
  quantile(bootstrap_distrib$stat,0.975))
```
]

.pull-right[
```{r,echo = FALSE,fig.height=4.5}
percentile_95ci = get_confidence_interval(bootstrap_distrib, level = 0.95, type = "percentile")
percentile_95lb = percentile_95ci[[1]]
percentile_95ub = percentile_95ci[[2]]

visualize(bootstrap_distrib,fill = "darkgreen") +
      shade_ci(endpoints = percentile_95ci,
                              fill = "blue",color = "blue",
                              alpha = 0.2) +
    # geom_vline(xintercept = p_hat, col = "green", size = 1, linetype = 2) +
    scale_x_continuous(breaks = sort(c(0.3,round(percentile_95lb,2),
                                       round(percentile_95ub,2), 0.8)),
                       limits = c(0.3, 0.8)) +
      labs(x = "Proportion of green pasta",
       y = "Frequency",
       title = "Simulated Bootstrap Distribution") +
    theme_bw(base_size = 20)
```
]

---

# Interpreting a 95% confidence interval

* Let's first check if the interval we built from our sample contains the true value.

  * Again, in reality we cannot do this check, here we simply show you that we are not loosing our mind. 
  
--

* In our example we have: $p = `r round(p,3)`$

--

.pull-left[

```{r, echo = FALSE, fig.height=4.5}
visualize(bootstrap_distrib,fill = "darkgreen") + 
    shade_ci(endpoints = percentile_95ci,
                              fill = "blue",color = "blue",
                              alpha = 0.2) +
    geom_vline(xintercept = p, col = "blue", size = 1.5, linetype = 1) +
    # geom_vline(xintercept = p_hat, col = "green", size = 1, linetype = 2) +
    scale_x_continuous(breaks = sort(c(0.3,round(percentile_95lb,2),round(p,2),round(percentile_95ub,2), 0.8)), 
                       limits = c(0.3, 0.8)) +
      labs(x = "Proportion of green pasta",
       y = "Frequency",
       title = "Simulated Bootstrap Distribution") +
    theme_bw(base_size = 20)

```

]

--

.pull-right[

</br>

* So yes, the true value (blue line) is indeed in our 95% interval! 

* Ok but, what if we had another sample from the begining, i.e. a different sample from the `bowl`.

  * Would $p$ be also contained in it?

]


---

# Interpreting a 95% confidence interval

* Let's repeatedly draw 100 different samples from our `bowl` and for each sample compute the associated 95% CI.

.pull-left[

```{r, echo=FALSE, fig.height=6}
library(purrr)
library(tidyr)
if(!file.exists("../rds/pasta_percentile_cis.rds")){
    set.seed(5)
    
    # Function to run infer pipeline
    bootstrap_pipeline <- function(sample_data){
        sample_data %>% 
            specify(formula = color ~ NULL, success = "green") %>% 
            generate(reps = 1000, type = "bootstrap") %>% 
            calculate(stat = "prop")
    }
    
    # Compute nested data frame with sampled data, sample proportions, all 
    # bootstrap replicates, and percentile_ci
    pasta_percentile_cis <- bowl %>% 
        mutate(color = as.factor(ifelse(color == "green","green","non-green"))) %>%
        rep_sample_n(size = 40, reps = 100, replace = FALSE) %>% 
        group_by(replicate) %>% 
        nest() %>% 
        mutate(sample_prop = map_dbl(data, ~mean(.x$color == "green"))) %>%
        # run infer pipeline on each nested tibble to generated bootstrap replicates
        mutate(bootstraps = map(data, bootstrap_pipeline)) %>% 
        group_by(replicate) %>% 
        # Compute 95% percentile CI's for each nested element
        mutate(percentile_ci = map(bootstraps, get_ci, type = "percentile", level = 0.95))
    
    # Save output to rds object
    saveRDS(object = pasta_percentile_cis, "../rds/pasta_percentile_cis.rds")
} else {
    pasta_percentile_cis <- readRDS("../rds/pasta_percentile_cis.rds")
}

# Identify if confidence interval captured true p
percentile_cis <- pasta_percentile_cis %>% 
    unnest(percentile_ci) %>% 
    mutate(captured = `2.5%` <= p & p <= `97.5%`)

# Plot them!
ggplot(percentile_cis) +
    geom_segment(aes(
        y = replicate, yend = replicate, x = `2.5%`, xend = `97.5%`, 
        alpha = factor(captured, levels = c("TRUE", "FALSE"))
    )) +
    # Removed point estimates since it doesn't necessarily act as center for 
    # percentile-based CI's
    # geom_point(aes(x = sample_prop, y = replicate, color = captured)) +
    labs(x = expression("Proportion of green pasta"), y = "Confidence Interval number", 
         alpha = "Contains Truth", 
         title = "How often a 95% Confidence Interval contains the true value?") +
    geom_vline(xintercept = p, color = "blue", size = 1.5) +
    geom_point(aes(y = replicate, x = sample_prop ,alpha = factor(captured, levels = c("TRUE", "FALSE")))) +
    coord_cartesian(xlim = c(0.2, 0.8)) + 
    theme_bw(base_size = 13) +
    theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(),
          panel.grid.minor.x = element_blank())
```

]

--

.pull-right[

</br>

* Looking at what we obtained:

  * There are only 5 of our CI which missed $p$!

  * In other words, 95 of our CI contains the true value $p$!

* That illustrate what a 95% Confidence Interval **is**.

]

---

# Recap Confidence Interval


* In order to construct a confidence interval, we need: 

--

  1. An underlying **sampling distribution of the estimate**, either boostrapped as in our example, or derived from theory.
  
--
  
  2. To specify a **confidence level**, like 90%, 95% etc. 
  
--

* For a 95% CI, we say that we are **95% confident that the interval contains the true parameter**

  * .small[If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter. As we saw in the previous slide.]

--

***Question*** 

  * How does the size of the confidence interval evolves when setting higher levels of confidence?
  * How does the size of the confidence interval evolves when the sample size increases?
  
---

# From Confidence Intervals to Hypothesis Testing

* Now that we’ve equipped ourselves with confidence intervals

* Let's see another common tool for statistical inference: hypothesis testing. 

* Just like confidence intervals, hypothesis tests are used to infer about a population using a sample.

* However, we’ll see that the framework for making such inferences is slightly different.

---

layout: false
class: title-slide-section-red, middle

# Hypothesis Testing

---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Is There Gender Discrimination In Promotions?

Let's start with a example.

.left-wide[
* We will use data from an [article](https://pdfs.semanticscholar.org/39f6/d40e907ff08af4ddd3280c2ceee55ee1ddb6.pdf) published in the *Journal of Applied Psychology* in 1974 which investigates whether female employees at Banks are discriminated against.

* 48 (male) supervisors were given *identical* candidate CVs - identical up to the first name, which was male or female. 

  * Each CV was "*in the form of a memorandum requesting a decision on the promotion of an employee to the position of branch manager*";

* **Hypothesis** we want to test : *Is there gender discrimination?*

* The data from the experiment are provided in the `promotions` dataset from the `moderndive` package.

]

--

.right-thin[

</br>
```{r}
library(moderndive)
promotions
```
]

---

# Conclusive evidence of discrimination

.pull-left[
```{r,echo= FALSE, fig.height=4.5}
ggplot(promotions, aes(x = gender, fill = decision)) +
  geom_bar(width = 0.75) +
  labs(x = "Gender of name on resume") +
  labs(title = "Promotion decision") +
  theme_bw(base_size = 18)
```

]

.pull-right[
</br>

```{r}
promotions %>% 
  group_by(gender, decision) %>% 
  summarize(n = n()) %>%
  mutate(proportion = 100* n / sum(n))
```
]

--


There is **29.2 percentage points difference** in favor of "men".

  * 87.5% of "men" were promoted.
  
  * 58.3% of "women" were promoted.

---

# Conclusive evidence of discrimination

.pull-left[
```{r,echo= FALSE, fig.height=4.5}
ggplot(promotions, aes(x = gender, fill = decision)) +
  geom_bar(width = 0.75) +
  labs(x = "Gender of name on resume") +
  labs(title = "Promotion decision") +
  theme_bw(base_size = 18)
```

]

.pull-right[
</br>

```{r}
promotions %>% 
  group_by(gender, decision) %>% 
  summarize(n = n()) %>%
  mutate(proportion = 100* n / sum(n))
```
]

--

***Question***: Is the 29%p advantage for men **conclusive evidence**? 

  * Could we have observed a 29% difference *by chance*?

---

# Hypothesis testing: Basic idea 

The idea of hypothesis tests is quite simple: 

</br>

--

1. We asusme that the hypothesis we want to test is true. 

  * In our case: we live in a world with no gender discrimination in promotions. 

--

1. Then we look how consistent are our data with this hypothetical world. 

  * In our case: We gonna ask how likely it is to get a 29%p difference in promotion rate if there is no discrimination. 
  

---


# Imposing A Hypothetical World: No Gender Discriminiation

.pull-left[
* Suppose we lived in a world without gender discrimination.

* The promotion decision would have been completely independent from gender.

  * i.e. the label `gender` in our dataframe would be meaningless.

* Let's randomly reassign `gender` to each row and see how this affects the result.

  * This is what has been done to obtain the `promotions_shuffled` dataset
]

--

.pull-right[

</br>

```{r}
head(bind_cols(promotions, promotions_shuffled))
```

* The `decision1` equals the original `decision` column...

* ... but the `gender` column has been suffled to obtain `gender1`. 

]

---

# Reshuffled Promotions

What does the promotions look like in our reshuffled sample? 

.pull-left[
```{r,echo=FALSE,fig.height = 4}
p1 = ggplot(promotions, aes(x = gender, fill = decision)) +
  geom_bar(width = 0.75) +
  theme_bw(base_size = 15) +
  theme(legend.position = "none") +
  labs(x = "Gender of resume name",title = "Original")
p2 = ggplot(promotions_shuffled, aes(x = gender, fill = decision)) +
  geom_bar(width = 0.75) +
  theme_bw(base_size = 15) +
  labs(x = "Gender of resume name",title = "Reshuffled")
cowplot::plot_grid(p1,p2,rel_widths = c(1,1.4))
```

]

.pull-right[
</br>

```{r}
promotions_shuffled %>% 
  group_by(gender, decision) %>% 
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n))
```

]

</br>

* The difference is much lower: 4.2%p.

---

# Sampling Variation?

* In our hypothetical world, the difference in promotion rate was only about 4.2%p.

* Can we answer our initial question about the existence of gender discrimination now?

--

* No, we must look at the role of **sampling variation**? 

  * What if we reshuffle once again, how different from 4.2%p the difference would be?
  
  * In other words, how representative of that hypothetical world is 4.2%?
  
  * Relatedly, how likely a 29%p difference is going to occur in such a world? 

* We need to know about the whole sampling distribution under the *no discrimination* hypothesis. 

--

* How? Just by redoing the reshuffling a large number of times, computing the difference each time.

---

# Sampling Variation in Reshuffling

Here is our sampling distribution in a world without discrimination. 

.pull-left[
```{r,echo = FALSE, fig.height=6}
set.seed(1)
null_distribution <- promotions %>% 
  # takes formula, defines success
  specify(formula = decision ~ gender,
          success = "promoted") %>% 
  # decisions are independent of gender
  hypothesize(null = "independence") %>% 
  # generate 1000 reshufflings of data
  generate(reps = 1000, type = "permute") %>% 
  # compute p_m - p_f from each reshuffle
  calculate(stat = "diff in props",
            order = c("male", "female"))

visualize(null_distribution, bins = 10, fill = "darkgreen") + 
  labs(title = "Sampling distribution", x = "Diff. in promotion rates") +
  xlim(-0.4, 0.4)+
  theme_bw(base_size = 20)
```
]

.pull-right[
</br>
```{r,echo = TRUE, eval = FALSE}
null_distribution <- promotions %>% 
  # takes formula, defines success
  specify(formula = decision ~ gender,
          success = "promoted") %>% 
  # decisions are independent of gender
  hypothesize(null = "independence") %>% 
  # generate 1000 reshufflings of data
  generate(reps = 1000, type = "permute") %>% 
  # compute p_m - p_f from each reshuffle
  calculate(stat = "diff in props",
            order = c("male", "female"))

visualize(null_distribution, bins = 10, fill = "darkgreen") +
  labs(title = "Sampling distribution", x = "Diff. in promotion rates") +
  theme_bw(base_size = 20)
```
]

---

# Comparing with the observed difference

Where does the observed difference of 29.2%p lie in this distribution?

.pull-left[
```{r,echo = FALSE, fig.height=6}
visualize(null_distribution, bins = 10, fill = "darkgreen") + 
  geom_vline(xintercept = 0.292, col = "red", size =1.5) +
  xlim(-0.4, 0.4)+
  labs(title = "Sampling distribution", x = "Diff. in promotion rates") +
  theme_bw(base_size = 20)
```
]

.pull-right[
</br>
* This distribution was generated under our **hypothetical** scenario: no discrimination.

* We see how sampling variation affects the difference in promotion rates.

* The red line denotes the *observed difference* in the **real world**.

* Now we can tell: how *likely* is it to observe a 29.2%p difference if no discrimination? 

]

---

# Recap

* We observed a 29%p difference in promotion rate between women and men in the real world.

* The question is whether in a hypothetical universe with no discrimination, 29%p is *likely* to occur.

* We concluded *rather not*, i.e. we tended to **reject** that hypothesis.

--

* You just conducted your first hypothesis test! 

  * We actually did a **permutation test**. We randomly reshuffled gender accross promotion decisions and checked if it makes a difference.

--

* Let's introduce the formal framework of hypothesis testing now. 

---

# Hypothesis Test Notation and Definitions

* In Hypothesis testing we compare two **competing hypothesis** about our parameter of interest. 
    * In the previous example:
        $$\begin{align}H_0:& p_m - p_f = 0\\H_A:& p_m - p_f > 0\end{align}$$

* $H_0$ stands for the **null hypothesis**, where *no effect* is observed. That's our hypothetical world from above.

* $H_A$ or $H_1$ is the **alternative** hypothesis. 
  * Here, we considered a *one-sided* alternative, saying that $p_m > p_f$, ie women are discriminated against.
  * The *two-sided* formulation is just $H_A: p_m - p_f \neq 0$

---

# Hypothesis Test Notation and Definitions

* A **test statistic** is a summary statistic which we use to summarise a certain aspect of our sample. 
  * In our previous case it was: $\hat{p}_m - \hat{p}_f$

* The **observed test statistic** is the number we get from our real world sample: 
$$\hat{p}_m - \hat{p}_f = 29.2\%$$

* The **null distribution** is the sampling distribution of our test statistic, assuming the Null hypothesis is **true**. 
  *  All the possible values that $\hat{p}_m - \hat{p}_f$ can take assuming there is no discrimination.

  * That's the distribution we have seen just before. 
 
---

# Null Distribution

.pull-left[
```{r,echo = FALSE,fig.height=6}
visualize(null_distribution, bins = 10, fill = "darkgreen") + 
  geom_vline(xintercept = 0.292, col = "red", size =1.5) +
  labs(x = "Diff. in promotion rates") +
  xlim(-0.4, 0.4)+
  theme_bw(base_size = 20)
```
]

.pull-right[
</br>
</br>

* This is the sampling distribution of $\hat{p}_m - \hat{p}_f$, assuming $H_0$ is true.

</br>

* The red line is the *observed test statistic*.
]

---

# Hypothesis Test Notation and Definitions

* The **p-value** is the probability of observing a test statistic *more extreme* than the one we obtained, assuming $H_0$ is true. `r emo::ji("thinking")`

  * How *strong* a piece of evidence is it to observe $\hat{p}_m - \hat{p}_f=29\%$ in a world where $p_m - p_f=0$ is assumed true? Very strong? Not so strong?

  * How many samples did we obtain that had a difference *greater* than 29%? Many, or not so many?

--

* ***Interpretation***: The lower the p-value, the less consistent our Null hypothesis is with the observed statistic.

--

* Ok but when do we decide to reject $H_0$ or not? 

---

# Hypothesis Test Notation and Definitions

* To decide wether we reject $H_0$ or not, we set a **significance level** for the test. 

* The **significance level** $\alpha$ is a *cutoff* on the p-value.

  * Common values are $\alpha = 1\%$, $5\%$, or $10\%$ .

--

* **Decision**: If the p-value falls below the cutoff $\alpha$, we **reject** the null hypothesis at the significance level $\alpha$.

--

* ***Interpretation***: If what we observe *is too unlikely to happen* under the Null, it means that this hypothesis is *likely to be false*.

--

* Let's illustrate how it works in our example. 

---

# Visualizing the P-value

.pull-left[
```{r,fig.height=6, echo = FALSE}
obs_diff_prop <- promotions %>%
  specify(decision ~ gender, success = "promoted") %>%
  calculate(stat = "diff in props", order = c("male", "female"))

visualize(null_distribution, bins = 10, fill ="darkgreen") + 
  shade_p_value(obs_stat = obs_diff_prop,
                size = 0.5,
                direction = "right", 
                fill = "red") +
  labs(x = "Diff. in promotion rates")+
  xlim(-0.4, 0.4)+
  theme_bw(base_size = 20)
```
]

.pull-right[

</br>

* The red area **is the p-value**!

* In our case *more extreme* means *bigger difference* (in favor of men).

* So the p-value measures the probability to the right of the red line.

* Is that a *big* or a *small* area?
]


---

# Obtaining the p-value and Deciding

* The red area, i.e. the p-value, equals **`r p_value`** in our case.

```{r}
p_value <- null_distribution %>%
  get_p_value(obs_stat = obs_diff_prop, direction = "right")
p_value$p_value
```

* In a world without discrimination, we would get $\hat{p_m} - \hat{p_f}$ superior (or equal) to 29.2%p only `r 100*p_value`% of the time. 

* So, we can reject $H_0$, i.e. the absence of discrimination, at the 5% significance level. 

  * We also say that $\hat{p_m} - \hat{p_f} = 29.2$%p is **significantly different from 0** at the 5% level. 
 
--

* **Question**: Suppose we had set $\alpha = 0.01 = 1\%$, would we have rejected the absence of discrimination at this level? 

---

# Testing Errors

Working with probabilities implies that sometimes, we make **errors**.

* A 29%p difference may be *unlikely* under $H_0$, but that **doesn't mean it's *impossible* to occur**.

  * In fact, such a difference (or higher) would occur in 1.9% of the cases.

* So, it may happen that we sometimes reject $H_0$, when in fact it was true.

  * Setting 5% significance level, you make sure it won't happen more than 5% of the time. 

---

# Testing Errors

In fact, in hypothesis testing, there are even **two types of errors** to make! 😲:

.pull-left[

![:scale 100%](../img/photos/gt_error_table_ht.png)

]

.pull-right[

* **Type I error**: We Reject a *true* Null.
  
* **Type II error**: We *fail* to reject a *wrong* Null.

]

* In practice, we choose the frequency of a Type I error by setting $\alpha$ and try to minimize the type II error.

* This is similar to a verdict reach in a court trial:

![:scale 48%](../img/photos/gt_error_table.png)

---

# Teaser for next week 

* Now you have all the tools to make **statistical inference** for real! 

* The regression tables no longer hold any secrets for you:

  * (OLS) Coefficients estimates

  * (Adjusted) $R^2$

  * Test-statistics

  * p-value

--

* Well, we will see this next week!

---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

