<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ScPoEconometrics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Florian Oswald, Gustave Kenedi and Pierre Villedieu" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="../css/scpo.css" type="text/css" />
    <link rel="stylesheet" href="../css/scpo-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ScPoEconometrics
## Confidence Intervals and Hypothesis Testing
### Florian Oswald, Gustave Kenedi and Pierre Villedieu
### SciencesPo Paris </br> 2020-04-07

---


layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---



# Recap from last week

* Sampling: ***sampling variation*** and ***distribution distributions***

* population, population parameter, sample statistic or point estimate, random sampling

* ***Unbiased estimator***: `\(\mathop{\mathbb{E}}[\hat{p}] = p\)`

* ***Central Limit Theorem***: as sample size increases, sampling distribution is more ***normally distributed***, centered around the ***population parameter***, and has a smaller ***standard error***

--

## Today: Deeper dive into ***statistical inference*** &lt;sup&gt;1&lt;/sup&gt;

.footnote[
[1]: This lecture is based on the wonderful [confidence interval](https://moderndive.com/8-confidence-intervals.html) and [hypothesis testing](https://moderndive.com/9-hypothesis-testing.html) chapters of [ModernDive](https://moderndive.com/)]

* Process of making claims about a population based on information from a sample

* *Confidence intervals*: providing plausible ***range*** of values

* *Hypothesis testing*: comparing statistics between groups

---

background-image: url(https://media.giphy.com/media/hXG93399r19vi/giphy.gif)
background-position: 18% 45%
background-size: 450px

# Back to reality (there goes gravity üòâ)

.pull-right[

* In real life we only get to take ***one*** sample from the population (not ***1000***!).

* Also, we obviously don't know the true population parameter, that's what we are interested in!

* So what on earth was all of this good for? Fun only?! üòß

]

--

&lt;br&gt;
&lt;br&gt;

* Even unobserved, we ***know*** that the sampling distribution does exist, and even better, we know how it behaves!

* Let's see what we can do with this...

---

layout: false
class: title-slide-section-red, middle

# Confidence Intervals

---

layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# From Point Estimates to Confidence Intervals

* Until now, we have only estimated ***point estimates*** from our samples: *sample means*, *sample proportions*, *regression coefficients*, etc.

--

* We know that this ***sample statistic*** differs from the ***true population parameter*** due to ***sampling variation***. 

--

* Rather than a point estimate, we could give a ***range of plausible values*** for the population parameter.

--

* This is precisely what a ***confidence interval*** (CI) provides.

---

# Constructing Confidence Intervals 

There are several approaches to constructing confidence intervals:
    
  1. *Theory*: use mathematical formulas (***Central Limit Theorem***) to derive the sampling distribution of our point estimate under certain conditions.

--

  1. *Simulation.* use the ***bootstrapping*** method to *reconstruct* the sampling distribution of our point estimate.
    
--
    
We'll focus on simulation to give you the intuition and come back to the maths approach next week.

--

But first: what is a *bootstrap*?

---

# Bootstrapping

* ***Key idea***: *pretend* the sample **is** the population, and repeatedly *resample with replacement* from it until obtaining a new sample of the same size as the original sample.

--

* Bootstrapping does ***not*** improve point estimation.

--

* But it yields a very good estimate of the ***standard error*** of the ***sampling distribution***.

---

background-image: url(https://media.giphy.com/media/7pHTiZYbAoq40/giphy.gif)
background-position: 50% 50%
background-size: 800px

---

# Back to Pasta



* As in real life, we have access to *only one random sample* from the entire population.

--

* So let's start by drawing one random sample of size `\(n = 40\)` from our bowl


```r
bowl &lt;- read.csv("https://www.dropbox.com/s/qpjsk0rfgc0gx80/pasta.csv?dl=1")

my_sample = bowl %&gt;%
  mutate(color = as.factor(ifelse(color == "green","green","non-green"))) %&gt;%
  rep_sample_n(size = 40) %&gt;%
  ungroup() %&gt;%
  select(pasta_ID, color) %&gt;%
  arrange(pasta_ID)
```

.pull-left[

```r
head(my_sample)
```

```
## # A tibble: 6 x 2
##   pasta_ID color    
##      &lt;int&gt; &lt;fct&gt;    
## 1       21 green    
## 2       83 green    
## 3       84 green    
## 4      117 non-green
## 5      161 green    
## 6      162 green
```
]

--

.pull-right[

```r
p_hat = mean(my_sample$color == "green")
p_hat
```

```
## [1] 0.65
```

The proportion of green pasta in this sample is: `\(\hat{p} = 0.65\)`.
]

---

# Boostrapping our Pasta Sample

How do we get a ***bootstrap sample***?

--
  
  1. Randomly pick ***one*** pasta from the sample and record the associated color.
  
--
  
  1. Put this pasta back in the sample.
  
--
  
  1. Repeat steps 1 and 2 39 times, i.e. until your new sample is of the same size as the original sample.

--

  1. Compute the proportion of green pasta in the bootstrap sample.

---

# Boostrapping our Pasta Sample

Here is one bootstrap sample: 

.pull-left[


```r
one_bootstrap = my_sample %&gt;%
  rep_sample_n(size = 40, replace = TRUE) %&gt;%
  arrange(pasta_ID)

head(one_bootstrap)
```

```
## # A tibble: 6 x 3
## # Groups:   replicate [1]
##   replicate pasta_ID color
##       &lt;int&gt;    &lt;int&gt; &lt;fct&gt;
## 1         1       21 green
## 2         1       21 green
## 3         1       21 green
## 4         1       21 green
## 5         1       83 green
## 6         1       84 green
```

```r
nrow(one_bootstrap)
```

```
## [1] 40
```

```r
mean(one_bootstrap$color == "green")
```

```
## [1] 0.725
```
]

--

.pull-right[

* All pasta are drawn from our initial sample.

* You can notice that:

  * Several pasta have been drawn multiple times! That's normal because we did ***resampling with replacement***.

  * The bootstrap sample size is equal to our initial sample size.
  
  * The proportion of green pasta is very similar to that in our sample.
]

  
---

# Obtaining the Bootstrap Distribution

* Repeating the whole procedure 1000 times, you will get 1000 bootstrap samples and 1000 bootstrap estimates!

* We use the `infer` package to ease the bootstrapping procedure.





```r
library(infer)

bootstrap_distrib = my_sample %&gt;% # take my random sample
  specify(response = color, success = "green") %&gt;% # specify the variable and level of interest
  generate(reps = 1000, type = "bootstrap") %&gt;% # generate 1000 bootstrap samples 
  calculate(stat = "prop") # calculate the proportion of green pasta for each
```

--

* Here are the first 3 rows:


```r
head(bootstrap_distrib,3)
```

```
## # A tibble: 3 x 2
##   replicate  stat
##       &lt;int&gt; &lt;dbl&gt;
## 1         1 0.75 
## 2         2 0.675
## 3         3 0.725
```

---

# Visualizing Our Simulated Distribution

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;

---

# Building a 95% Confidence Interval

* Let's define an interval that covers 95% of our bootstrapped estimates.

* We get it by cutting the boostrap distribution at the 2.5% and 97.5% quantiles.

--

* It defines our 95% CI as: `\(CI^B = [q_{2,5\%} ; q_{97,5\%}]\)`

* In our case gives: `\([0.5 ; 0.8]\)`


```r
quantile(bootstrap_distrib$stat,0.025)
```

```
## 2.5% 
##  0.5
```

```r
quantile(bootstrap_distrib$stat,0.975)
```

```
## 97.5% 
##   0.8
```

---

# Building a 95% Confidence Interval: Visually

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-11-1.svg" style="display: block; margin: auto;" /&gt;

--

* Does the interval contain the true population proportion?

---

# Building a 95% Confidence Interval: Visually

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;

--

True population parameter (blue line) is indeed in our 95% interval! Will it always be?

---

# Interpreting a 95% confidence interval

* Let's repeatedly draw 100 different samples from our `bowl` and for each sample compute the associated 95% CI.

.pull-left[

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

&lt;/br&gt;

* Looking at what we obtained:

  * There are only 4 of our CIs which missed `\(p\)`!

  * In other words, 96 of our CIs contain the true value `\(p\)`!

* That illustrate what a 95% Confidence Interval **is**.

]

---

# Recap Confidence Interval


* In order to construct a confidence interval, we need: 

--

  1. An underlying **sampling distribution of the estimate**, either boostrapped as in our example, or derived from theory.
  
--
  
  2. To specify a **confidence level**, like 90%, 95% etc. 
  
--

&gt; *Precise interpretation:* If we repeated our sampling procedure ***a large number of times***, we ***expect about 95%*** of the resulting confidence intervals to capture the value of the population parameter.

--

&gt; *Short-hand interpretation:* We are ***95% ‚Äúconfident‚Äù*** that a 95% confidence interval captures the value of the population parameter.

--

***Questions:*** 

  * How does the size of the confidence interval evolves when setting higher levels of confidence?
  * How does the size of the confidence interval evolves when the sample size increases?
  
---

# From Confidence Intervals to Hypothesis Testing

* *Confidence intervals* can be thought of as an extension of *point estimation*.

--

* What if we want to ***compare*** a sample statistic for two groups?
  
  * *Example*: differences in average wages between men and women.
  
--

* These comparisons are the realm of ***hypothesis testing***.

--

* Just like confidence intervals, hypothesis tests are used to make claims about a population based on information from a sample.

* However, we‚Äôll see that the framework for making such inferences is slightly different.

---

layout: false
class: title-slide-section-red, middle

# Hypothesis Testing

---
layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# Is There Gender Discrimination In Promotions?

Let's start with a example.

.left-wide[
* We will use data from an [article](https://pdfs.semanticscholar.org/39f6/d40e907ff08af4ddd3280c2ceee55ee1ddb6.pdf) published in the *Journal of Applied Psychology* in 1974 which investigates whether female employees at Banks are discriminated against.

* 48 (male) supervisors were given *identical* candidate CVs, differing only with respect to the first name, which was male or female.

  * Each CV was "*in the form of a memorandum requesting a decision on the promotion of an employee to the position of branch manager.*"

* ***Hypothesis*** we want to test: *Is there gender discrimination?*

* The data from the experiment are provided in the `promotions` dataset from the `moderndive` package.

]

--

.right-thin[

&lt;/br&gt;

```r
library(moderndive)
promotions
```

```
## # A tibble: 48 x 3
##       id decision gender
##    &lt;int&gt; &lt;fct&gt;    &lt;fct&gt; 
##  1     1 promoted male  
##  2     2 promoted male  
##  3     3 promoted male  
##  4     4 promoted male  
##  5     5 promoted male  
##  6     6 promoted male  
##  7     7 promoted male  
##  8     8 promoted male  
##  9     9 promoted male  
## 10    10 promoted male  
## # ‚Ä¶ with 38 more rows
```
]

---

# Evidence of Discrimination?


.pull-left[

How many men and women were offered a promotion (and not)?


```r
promotions %&gt;% 
  group_by(gender, decision) %&gt;% 
  tally %&gt;%
  mutate(percentage = 100* n / sum(n))
```

```
## # A tibble: 4 x 4
## # Groups:   gender [2]
##   gender decision     n percentage
##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 male   not          3       12.5
## 2 male   promoted    21       87.5
## 3 female not         10       41.7
## 4 female promoted    14       58.3
```

There is a ***29.2 percentage points difference*** in promotions between men and women.
]

--

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-16-1.svg" style="display: block; margin: auto;" /&gt;
]

--

***Question***: Is this difference ***conclusive evidence*** of differences in promotion rates between men and women? Could such a difference have been observed ***by chance***?

---

# Hypothesis testing: Basic Framework

1. We assume that the hypothesis we want to test is true.

  * In our case: no gender discrimination in promotions.

--

1. Then we evaluate how consistent the data is with this hypothetical world. 

  * In our case: how likely it is to observe a 29 percentage point difference in promotion rates in a world with no discrimination. 

---


# Imposing A Hypothetical World: No Gender Discrimination

* Suppose we lived in a world without gender discrimination.

* The promotion decision would be completely ***independent*** from gender.

* Let's randomly reassign `gender` to each row and see how this affects the result.

--

.pull-left[


```
## # A tibble: 6 x 6
##      id decision gender   id1 decision1 gender1
##   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;  
## 1     1 promoted male       1 promoted  female 
## 2     2 promoted male       2 promoted  female 
## 3     3 promoted male       3 promoted  male   
## 4     4 promoted male       4 promoted  female 
## 5     5 promoted male       5 promoted  male   
## 6     6 promoted male       6 promoted  male
```
]

.pull-right[
* The `decision1` equals the original `decision` column...

* ... but the `gender` column has been suffled to obtain `gender1`. 
]

---

# Reshuffled Promotions

.pull-left[
How do the promotion rates look like in our reshuffled sample?


```r
promotions_shuffled %&gt;% 
  group_by(gender, decision) %&gt;% 
  tally() %&gt;%
  mutate(proportion = n / sum(n))
```

```
## # A tibble: 4 x 4
## # Groups:   gender [2]
##   gender decision     n proportion
##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 male   not          6      0.25 
## 2 male   promoted    18      0.75 
## 3 female not          7      0.292
## 4 female promoted    17      0.708
```

* The difference is much lower: ***4.2 percentage points***!
]

--

.pull-right[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-19-1.svg" style="display: block; margin: auto;" /&gt;

]



---

# Sampling Variation

* In our hypothetical world, the difference in promotion rate was only about 4.2 percentage points.

* Can we answer our initial question about the existence of gender discrimination now?

--

* No, we must look at the role of ***sampling variation***!

  * What if we reshuffle once again, how different from 4.2%p would the difference be?

--
  
  * In other words, how representative of that hypothetical world is 4.2%p?
  
--
  
  * Relatedly, how likely is a 29%p difference to occur in such a world?
  
--

* We need to know about the whole sampling distribution under the *no discrimination* hypothesis.

--

* How? Just by redoing the reshuffling a large number of times, and computing the difference each time.

---

# Sampling Variation with 1000 Reshufflings

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-20-1.svg" style="display: block; margin: auto;" /&gt;

---

# Sampling Variation with 1000 Reshufflings

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-21-1.svg" style="display: block; margin: auto;" /&gt;

How ***likely*** is it to observe a 29.2 percentage point difference in a world with no discrimination?

---

# Recap

* We observed a 29 percentage point difference in promotion rate between women and men in the real world.

--

* The question is whether in a hypothetical universe with no discrimination, 29%p is ***likely*** to occur.

--

* We concluded ***rather not***, i.e. we tended to ***reject*** the no discrimination hypothesis.

--

* We just conducted our first hypothesis test! 

  * We actually did a ***permutation test***. We randomly reshuffled gender accross promotion decisions and checked if it makes a difference.

--

* Let's introduce the formal framework of hypothesis testing now.

---

# Hypothesis Test Notation and Definitions

* A ***hypothesis test*** consists of a test between ***two competing hypotheses*** about the population parameter:

--

  * The ***null hypothesis*** `\((H_0)\)`: generally hypothesis of no difference;
--
  
  * The ***alternative hypothesis*** `\((H_A \textrm{or }H_1)\)`: the research hypothesis.

--

* In the previous example:
`$$\begin{align}H_0&amp;: p_m - p_f = 0\\H_A&amp;: p_m - p_f &gt; 0,\end{align}$$`
where `\(p_m =\)` promotion rate of men, and `\(p_f =\)` promotion rate of women.

--

  * Here, we considered a *one-sided* alternative, saying that `\(p_m &gt; p_f\)`, i.e. women are discriminated against.
  * The *two-sided* formulation is just `\(H_A: p_m - p_f \neq 0\)`

---

# Hypothesis Test Notation and Definitions

* ***Test statistic***: *point estimate/sample statistic* formula used for hypothesis testing. 

--

  * *In our previous case*: `\(\hat{p}_m - \hat{p}_f\)`.

--

* ***Observed test statistic***: value of the test statistic that we observed in real life.

--

  * *In our previous case*: `\(\hat{p}_m - \hat{p}_f = 29.2\)` percentage points.
  
--

* ***Null distribution***: sampling distribution of the test statistic *assuming the null hypothesis `\(H_0\)` is true*.

--
  *  *In our previous case*: All the possible values that `\(\hat{p}_m - \hat{p}_f\)` can take assuming there is no discrimination.
  * That's the distribution we have seen just before.
 
---

# Null Distribution

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-22-1.svg" style="display: block; margin: auto;" /&gt;

---

# Hypothesis Test Notation and Definitions

&gt; ### ***p-value:*** probability of observing a test statistic *more extreme* than the one we obtained, assuming `\(H_0\)` is true. ü§î

  * How *strong* a piece of evidence is it to observe `\(\hat{p}_m - \hat{p}_f=29\)` percentage points in a world where `\(p_m - p_f=0\)` is assumed true? Very strong? Not so strong?

  * How many samples did we obtain that had a difference *greater* than 29%p? Many, or not so many?

--

* ***Interpretation***: The lower the p-value, the *less consistent our null hypothesis is with the observed statistic*.

--

* Ok but when do we decide to reject `\(H_0\)` or not? 

---

# Hypothesis Test Notation and Definitions

* To decide wether we reject `\(H_0\)` or not, we set a ***significance level*** for the test.

--

* The ***significance level*** `\(\alpha\)` is a *cutoff* on the p-value.

  * Common values are `\(\alpha = 1\%\)`, `\(5\%\)`, or `\(10\%\)` .

--

* ***Decision***: If the p-value falls below the cutoff `\(\alpha\)`, we ***reject*** the null hypothesis at the significance level `\(\alpha\)`.

--

* ***Interpretation***: If what we observe *is too unlikely to happen* under the null hypothesis, it means that this hypothesis is ***likely to be false***.

--

* Let's illustrate how it works in our example. 

---

# Visualizing the P-value

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-23-1.svg" style="display: block; margin: auto;" /&gt;

--

The p-value measures the probability of being to the right of the red line.

]


---

# Obtaining the p-value and Deciding

* The red areas corresponds to the share of ***simulated differences in promotion rates under the null hypothesis*** that exceed the ***observed test statistic*** (0.292).


```r
p_value &lt;- mean(null_distribution$stat &gt;= 0.292)
p_value
```

```
## [1] 0.007
```

* In a world without discrimination, we would get `\(\hat{p_m} - \hat{p_f}\)` superior (or equal) to 29.2%p only 0.7% of the time. 
--

* So, we can reject `\(H_0\)`, i.e. the absence of discrimination, at the 5% significance level. 

  * We also say that `\(\hat{p_m} - \hat{p_f} = 29.2\)`%p is ***statistically significantly different from 0*** at the 5% level. 
 
--

* ***Question***: Suppose we had set `\(\alpha = 0.01 = 1\%\)`, would we have rejected the absence of discrimination at this level? 

---

# Testing Errors

Working with probabilities implies that sometimes, we make **errors**.

--

* A 29%p difference may be *unlikely* under `\(H_0\)`, but that **doesn't mean it's *impossible* to occur**.

  * In fact, such a difference (or higher) would occur in 1.9% of the cases.
  
--

* So, it may happen that we sometimes reject `\(H_0\)`, when in fact it was true.

  * Setting 5% significance level, you make sure it won't happen more than 5% of the time. 

---

# Testing Errors

In hypothesis testing, there are ***two types of errors***:

.pull-left[

![:scale 100%](../img/photos/gt_error_table_ht.png)

]

.pull-right[

***Type I error***: reject the null hypothesis when in fact it was true.
  
***Type II error***: don't reject the null hypothesis when in fact it was false.

]

* In practice, we choose the frequency of a Type I error by setting `\(\alpha\)` and try to minimize the type II error.

---

# How does all of this relate to regression analysis? 

* Now you have all the tools to make ***statistical inference*** for real!

--

* Regression analysis is based on a ***sample*** of data.

--

* So your ***regression coefficient*** is subject to ***sampling variation***, it's not the true population coefficient.

--

* ***Question***: Is the estimated effect statistically significantly different from 0?

--

* The answer in the next episode of *Introduction to Econometrics with R*! üêµ

---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| &lt;a href="mailto:florian.oswald@sciencespo.fr"&gt;.ScPored[&lt;i class="fa fa-paper-plane fa-fw"&gt;&lt;/i&gt;]               | florian.oswald@sciencespo.fr       |
| &lt;a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Slides |
| &lt;a href="https://scpoecon.github.io/ScPoEconometrics"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Book |
| &lt;a href="http://twitter.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-twitter fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                         |
| &lt;a href="http://github.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-github fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                       |
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../js/ru_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
