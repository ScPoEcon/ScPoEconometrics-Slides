<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ScPoEconometrics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Florian Oswald, Gustave Kenedi and Pierre Villedieu" />
    <script src="libs/jquery/jquery.min.js"></script>
    <script src="libs/elevate-section-attrs/elevate-section-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="..\css\scpo.css" type="text/css" />
    <link rel="stylesheet" href="..\css\scpo-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ScPoEconometrics
## Confidence Intervals and Hypothesis Testing
### Florian Oswald, Gustave Kenedi and Pierre Villedieu
### SciencesPo Paris </br> 2020-04-07

---


layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---



# Recap from last week?


* Last week we learned about **sampling distributions**

--

* We took repeated **random samples** from a our **population** (i.e. our pasta bowl)

  * For each sample, we computed the **point estimate** `\(\hat{p}\)` of the proportion of green pasta.

--

* The resulting distribution is called **sampling distribution**

  * It is **normally shaped** around the **population parameter**.
  
  * It becomes *narrower* when considering samples of bigger size.
--

* Looking at this sampling distribution, it seemed fairly easy to infer what the true proportion of green pasta was. 

---

background-image: url(https://media.giphy.com/media/hXG93399r19vi/giphy.gif)
background-position: 18% 45%
background-size: 450px

# Back to reality

.pull-right[

* In real life we only get to take **one** sample from the population.

  * So nobody could ever take **1000** samples from the population!

* Also, we obviously don't know about the true proportion of green pasta!
  
  * That's what we are interested in. 

* So what on earth was all of this good for? Fun Only?! ðŸ˜§

]


* Well, even unobserved, we **know** that the sampling distribution does exist, and even better, we know how it behaves!

* Let's see what we can do with this...

---

layout: false
class: title-slide-section-red, middle

# Confidence Intervals

---

layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# From Point Estimate to Confidence Intervals

* From now, we only get **point estimates** from our samples: either simple statistics like *means* or *proportions* or *regression coefficients*

--

* We know that this sample estimate differs from the true population parameter because of sampling variation. 

--

* Rather than a point estimate, we could give a range of plausible values for the population parameter. 

--

* That is precisely what a **Confidence Interval** (CI) provides.

---

# Constructing Confidence Intervals 

* There are several approaches to construct CI's:
    
  1. *Theory*: Use mathematical formulas (**Central Limit Theorem**) to derive the sampling distributions of our point estimates under certain conditions.

--

  1. *Simulation.* We can use the **Bootstrap** method to *reconstruct* the sampling distribution.
    
--
    
* We'll focus on the bootstrap to give you the intuition and come back to the maths approach later (in the next course).

--

* But first: what is a *bootstrap*?

---

background-image: url(https://upload.wikimedia.org/wikipedia/commons/c/c3/Gustave_Dor%C3%A9_-_Baron_von_M%C3%BCnchhausen_-_037.jpg)
background-position: 20% 70%
background-size: 350px

# Baron von MÃ¼nchhausen (not)

.pull-right[

* The ethymology of *bootstrapping* is apparently [wrongly attributed](https://en.wikipedia.org/wiki/Bootstrapping) to the Baron of MÃ¼nchhausen (who pulled himself out of the swamp by his own pigtail)

* The idea is to *pull oneself up by one's own bootstraps*.

* It sounds as if one had super-powers.

* It's a simple but powerful idea. 
 
 * We just *pretend* that our sample **is** the population. Then we repeatedly *resample* from it. 

]


---

background-image: url(https://media.giphy.com/media/7pHTiZYbAoq40/giphy.gif)
background-position: 50% 50%
background-size: 800px

---

# Our initial pasta sample



* As in real life, we have access to *only one random sample* from the entire population.

.pull-left[
* So let's start by drawing one random sample of size `\(n = 40\)` from our bowl:


```r
my_sample = bowl %&gt;%
  mutate(color = as.factor(ifelse(color == "green","green","non-green"))) %&gt;%
  rep_sample_n(size = sample_size) %&gt;%
  ungroup() %&gt;%
  select(pasta_ID, color) %&gt;%
  arrange(pasta_ID)
```
]

.pull-right[

```r
head(my_sample)
```

```
## # A tibble: 6 x 2
##   pasta_ID color    
##      &lt;int&gt; &lt;fct&gt;    
## 1        8 green    
## 2       54 non-green
## 3       71 green    
## 4       85 green    
## 5       92 non-green
## 6      122 green
```
]

&lt;/br&gt;

* The proportion of green pasta in this sample is: `\(\hat{p} = 0.55\)`.

```r
(p_hat = mean(my_sample$color=="green"))
```

```
## [1] 0.55
```

---

# Boostrapping our pasta sample

How do we get 1 **bootstrap sample**:
  
  1. Randomly pick one pasta from your sample and record the associated color.
  
  1. Put this pasta back in your sample.
  
  1. Repeat step 1 to 2 39 times (i.e. `\(n-1\)`)

  1. Compute the proportion of green pasta from your bootstrap sample

---

# Boostrapping our pasta sample

Here is one bootstrap sample: 

.pull-left[


```r
one_bootstrapp = my_sample %&gt;%
  rep_sample_n(size = 40, replace = TRUE) %&gt;%
  arrange(pasta_ID)
```


```
## # A tibble: 40 x 3
## # Groups:   replicate [1]
##    replicate pasta_ID color    
##        &lt;int&gt;    &lt;int&gt; &lt;fct&gt;    
##  1         1       54 non-green
##  2         1       71 green    
##  3         1      122 green    
##  4         1      123 non-green
##  5         1      128 non-green
##  6         1      128 non-green
##  7         1      146 green    
##  8         1      149 green    
##  9         1      149 green    
## 10         1      184 green    
## # ... with 30 more rows
```
]

--

.pull-right[

* All pasta have been draw from our initial sample

* You can notice that:

  * Several pasta have been drawn multiple times (ID=128,149,...)! That's normal.

  * The bootstrap sample size is equal to our initial sample size. 

]

  
---

# Getting the Bootstrap distribution

* Repeating the whole procedure 1000 times, you will get 1000 bootstrap samples and 1000 bootstrapped estimates! 

* We use the `infer` package that makes the whole bootstrapping procedure easier. 





```r
bootstrap_distrib = my_sample %&gt;% # take my random sample
  specify(response = color, success = "green") %&gt;% # specify the variable and level of interest
  generate(reps = 1000, type = "bootstrap") %&gt;% # generate 1000 bootstrap samples 
  calculate(stat = "prop") # calculate the proportion of green pasta for each
```

--

.pull-left[

* Here is the first 3 rows:


```r
head(bootstrap_distrib,3)
```

```
## # A tibble: 3 x 2
##   replicate  stat
##       &lt;int&gt; &lt;dbl&gt;
## 1         1 0.5  
## 2         2 0.475
## 3         3 0.5
```
]

--

.pull-right[

&lt;/br&gt;

***Question***

* What 0.475 is referring to? 

* How many rows has this dataframe? 
]


---

# Visualizing Our Simulated Distribution

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-10-1.svg" style="display: block; margin: auto;" /&gt;

---

# Building a 95% confidence interval

* Let's define an interval that covers 95% of our bootstrapped estimates.

* We get it by cutting the boostrap distribution at the 2.5% and 97.5% quantiles

* It defines our 95% CI as: `\(CI^B = [q_{2,5\%} ; q_{97,5\%}]\)`

.pull-left[

&lt;/br&gt;

* In our case gives: `\([0.4 ; 0.7]\)`


```r
c(quantile(bootstrap_distrib$stat,0.025),
  quantile(bootstrap_distrib$stat,0.975))
```

```
##  2.5% 97.5% 
##   0.4   0.7
```
]

.pull-right[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;
]

---

# Interpreting a 95% confidence interval

* Let's first check if the interval we built from our sample contains the true value.

  * Again, in reality we cannot do this check, here we simply show you that we are not loosing our mind. 
  
--

* In our example we have: `\(p = 0.494\)`

--

.pull-left[

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

&lt;/br&gt;

* So yes, the true value (blue line) is indeed in our 95% interval! 

* Ok but, what if we had another sample from the begining, i.e. a different sample from the `bowl`.

  * Would `\(p\)` be also contained in it?

]


---

# Interpreting a 95% confidence interval

* Let's repeatedly draw 100 different samples from our `bowl` and for each sample compute the associated 95% CI.

.pull-left[

&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-14-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

&lt;/br&gt;

* Looking at what we obtained:

  * There are only 5 of our CI which missed `\(p\)`!

  * In other words, 95 of our CI contains the true value `\(p\)`!

* That illustrate what a 95% Confidence Interval **is**.

]

---

# Recap Confidence Interval


* In order to construct a confidence interval, we need: 

--

  1. An underlying **sampling distribution of the estimate**, either boostrapped as in our example, or derived from theory.
  
--
  
  2. To specify a **confidence level**, like 90%, 95% etc. 
  
--

* For a 95% CI, we say that we are **95% confident that the interval contains the true parameter**

  * .small[If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter. As we saw in the previous slide.]

--

***Question*** 

  * How does the size of the confidence interval evolves when setting higher levels of confidence?
  * How does the size of the confidence interval evolves when the sample size increases?
  
---

# From Confidence Intervals to Hypothesis Testing

* Now that weâ€™ve equipped ourselves with confidence intervals

* Let's see another common tool for statistical inference: hypothesis testing. 

* Just like confidence intervals, hypothesis tests are used to infer about a population using a sample.

* However, weâ€™ll see that the framework for making such inferences is slightly different.

---

layout: false
class: title-slide-section-red, middle

# Hypothesis Testing

---
layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# Is There Gender Discrimination In Promotions?

Let's start with a example.

.left-wide[
* We will use data from an [article](https://pdfs.semanticscholar.org/39f6/d40e907ff08af4ddd3280c2ceee55ee1ddb6.pdf) published in the *Journal of Applied Psychology* in 1974 which investigates whether female employees at Banks are discriminated against.

* 48 (male) supervisors were given *identical* candidate CVs - identical up to the first name, which was male or female. 

  * Each CV was "*in the form of a memorandum requesting a decision on the promotion of an employee to the position of branch manager*";

* **Hypothesis** we want to test : *Is there gender discrimination?*

* The data from the experiment are provided in the `promotions` dataset from the `moderndive` package.

]

--

.right-thin[

&lt;/br&gt;

```r
library(moderndive)
promotions
```

```
## # A tibble: 48 x 3
##       id decision gender
##    &lt;int&gt; &lt;fct&gt;    &lt;fct&gt; 
##  1     1 promoted male  
##  2     2 promoted male  
##  3     3 promoted male  
##  4     4 promoted male  
##  5     5 promoted male  
##  6     6 promoted male  
##  7     7 promoted male  
##  8     8 promoted male  
##  9     9 promoted male  
## 10    10 promoted male  
## # ... with 38 more rows
```
]

---

# Conclusive evidence of discrimination

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-16-1.svg" style="display: block; margin: auto;" /&gt;

]

.pull-right[
&lt;/br&gt;


```r
promotions %&gt;% 
  group_by(gender, decision) %&gt;% 
  summarize(n = n()) %&gt;%
  mutate(proportion = 100* n / sum(n))
```

```
## # A tibble: 4 x 4
## # Groups:   gender [2]
##   gender decision     n proportion
##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 male   not          3       12.5
## 2 male   promoted    21       87.5
## 3 female not         10       41.7
## 4 female promoted    14       58.3
```
]

--


There is **29.2 percentage points difference** in favor of "men".

  * 87.5% of "men" were promoted.
  
  * 58.3% of "women" were promoted.

---

# Conclusive evidence of discrimination

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-18-1.svg" style="display: block; margin: auto;" /&gt;

]

.pull-right[
&lt;/br&gt;


```r
promotions %&gt;% 
  group_by(gender, decision) %&gt;% 
  summarize(n = n()) %&gt;%
  mutate(proportion = 100* n / sum(n))
```

```
## # A tibble: 4 x 4
## # Groups:   gender [2]
##   gender decision     n proportion
##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 male   not          3       12.5
## 2 male   promoted    21       87.5
## 3 female not         10       41.7
## 4 female promoted    14       58.3
```
]

--

***Question***: Is the 29%p advantage for men **conclusive evidence**? 

  * Could we have observed a 29% difference *by chance*?

---

# Hypothesis testing: Basic idea 

The idea of hypothesis tests is quite simple: 

&lt;/br&gt;

--

1. We asusme that the hypothesis we want to test is true. 

  * In our case: we live in a world with no gender discrimination in promotions. 

--

1. Then we look how consistent are our data with this hypothetical world. 

  * In our case: We gonna ask how likely it is to get a 29%p difference in promotion rate if there is no discrimination. 
  

---


# Imposing A Hypothetical World: No Gender Discriminiation

.pull-left[
* Suppose we lived in a world without gender discrimination.

* The promotion decision would have been completely independent from gender.

  * i.e. the label `gender` in our dataframe would be meaningless.

* Let's randomly reassign `gender` to each row and see how this affects the result.

  * This is what has been done to obtain the `promotions_shuffled` dataset
]

--

.pull-right[

&lt;/br&gt;


```r
head(bind_cols(promotions, promotions_shuffled))
```

```
## # A tibble: 6 x 6
##      id decision gender   id1 decision1 gender1
##   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;  
## 1     1 promoted male       1 promoted  female 
## 2     2 promoted male       2 promoted  female 
## 3     3 promoted male       3 promoted  male   
## 4     4 promoted male       4 promoted  female 
## 5     5 promoted male       5 promoted  male   
## 6     6 promoted male       6 promoted  male
```

* The `decision1` equals the original `decision` column...

* ... but the `gender` column has been suffled to obtain `gender1`. 

]

---

# Reshuffled Promotions

What does the promotions look like in our reshuffled sample? 

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-21-1.svg" style="display: block; margin: auto;" /&gt;

]

.pull-right[
&lt;/br&gt;


```r
promotions_shuffled %&gt;% 
  group_by(gender, decision) %&gt;% 
  summarize(n = n()) %&gt;%
  mutate(proportion = n / sum(n))
```

```
## # A tibble: 4 x 4
## # Groups:   gender [2]
##   gender decision     n proportion
##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 male   not          6      0.25 
## 2 male   promoted    18      0.75 
## 3 female not          7      0.292
## 4 female promoted    17      0.708
```

]

&lt;/br&gt;

* The difference is much lower: 4.2%p.

---

# Sampling Variation?

* In our hypothetical world, the difference in promotion rate was only about 4.2%p.

* Can we answer our initial question about the existence of gender discrimination now?

--

* No, we must look at the role of **sampling variation**? 

  * What if we reshuffle once again, how different from 4.2%p the difference would be?
  
  * In other words, how representative of that hypothetical world is 4.2%?
  
  * Relatedly, how likely a 29%p difference is going to occur in such a world? 

* We need to know about the whole sampling distribution under the *no discrimination* hypothesis. 

--

* How? Just by redoing the reshuffling a large number of times, computing the difference each time.

---

# Sampling Variation in Reshuffling

Here is our sampling distribution in a world without discrimination. 

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-23-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;/br&gt;

```r
null_distribution &lt;- promotions %&gt;% 
  # takes formula, defines success
  specify(formula = decision ~ gender,
          success = "promoted") %&gt;% 
  # decisions are independent of gender
  hypothesize(null = "independence") %&gt;% 
  # generate 1000 reshufflings of data
  generate(reps = 1000, type = "permute") %&gt;% 
  # compute p_m - p_f from each reshuffle
  calculate(stat = "diff in props",
            order = c("male", "female"))

visualize(null_distribution, bins = 10, fill = "darkgreen") +
  labs(title = "Sampling distribution", x = "Diff. in promotion rates") +
  theme_bw(base_size = 20)
```
]

---

# Comparing with the observed difference

Where does the observed difference of 29.2%p lie in this distribution?

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-25-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;/br&gt;
* This distribution was generated under our **hypothetical** scenario: no discrimination.

* We see how sampling variation affects the difference in promotion rates.

* The red line denotes the *observed difference* in the **real world**.

* Now we can tell: how *likely* is it to observe a 29.2%p difference if no discrimination? 

]

---

# Recap

* We observed a 29%p difference in promotion rate between women and men in the real world.

* The question is whether in a hypothetical universe with no discrimination, 29%p is *likely* to occur.

* We concluded *rather not*, i.e. we tended to **reject** that hypothesis.

--

* You just conducted your first hypothesis test! 

  * We actually did a **permutation test**. We randomly reshuffled gender accross promotion decisions and checked if it makes a difference.

--

* Let's introduce the formal framework of hypothesis testing now. 

---

# Hypothesis Test Notation and Definitions

* In Hypothesis testing we compare two **competing hypothesis** about our parameter of interest. 
    * In the previous example:
        `$$\begin{align}H_0:&amp; p_m - p_f = 0\\H_A:&amp; p_m - p_f &gt; 0\end{align}$$`

* `\(H_0\)` stands for the **null hypothesis**, where *no effect* is observed. That's our hypothetical world from above.

* `\(H_A\)` or `\(H_1\)` is the **alternative** hypothesis. 
  * Here, we considered a *one-sided* alternative, saying that `\(p_m &gt; p_f\)`, ie women are discriminated against.
  * The *two-sided* formulation is just `\(H_A: p_m - p_f \neq 0\)`

---

# Hypothesis Test Notation and Definitions

* A **test statistic** is a summary statistic which we use to summarise a certain aspect of our sample. 
  * In our previous case it was: `\(\hat{p}_m - \hat{p}_f\)`

* The **observed test statistic** is the number we get from our real world sample: 
`$$\hat{p}_m - \hat{p}_f = 29.2\%$$`

* The **null distribution** is the sampling distribution of our test statistic, assuming the Null hypothesis is **true**. 
  *  All the possible values that `\(\hat{p}_m - \hat{p}_f\)` can take assuming there is no discrimination.

  * That's the distribution we have seen just before. 
 
---

# Null Distribution

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-26-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;/br&gt;
&lt;/br&gt;

* This is the sampling distribution of `\(\hat{p}_m - \hat{p}_f\)`, assuming `\(H_0\)` is true.

&lt;/br&gt;

* The red line is the *observed test statistic*.
]

---

# Hypothesis Test Notation and Definitions

* The **p-value** is the probability of observing a test statistic *more extreme* than the one we obtained, assuming `\(H_0\)` is true. ðŸ¤”

  * How *strong* a piece of evidence is it to observe `\(\hat{p}_m - \hat{p}_f=29\%\)` in a world where `\(p_m - p_f=0\)` is assumed true? Very strong? Not so strong?

  * How many samples did we obtain that had a difference *greater* than 29%? Many, or not so many?

--

* ***Interpretation***: The lower the p-value, the less consistent our Null hypothesis is with the observed statistic.

--

* Ok but when do we decide to reject `\(H_0\)` or not? 

---

# Hypothesis Test Notation and Definitions

* To decide wether we reject `\(H_0\)` or not, we set a **significance level** for the test. 

* The **significance level** `\(\alpha\)` is a *cutoff* on the p-value.

  * Common values are `\(\alpha = 1\%\)`, `\(5\%\)`, or `\(10\%\)` .

--

* **Decision**: If the p-value falls below the cutoff `\(\alpha\)`, we **reject** the null hypothesis at the significance level `\(\alpha\)`.

--

* ***Interpretation***: If what we observe *is too unlikely to happen* under the Null, it means that this hypothesis is *likely to be false*.

--

* Let's illustrate how it works in our example. 

---

# Visualizing the P-value

.pull-left[
&lt;img src="CI_and_hypothesis_test_files/figure-html/unnamed-chunk-27-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[

&lt;/br&gt;

* The red area **is the p-value**!

* In our case *more extreme* means *bigger difference* (in favor of men).

* So the p-value measures the probability to the right of the red line.

* Is that a *big* or a *small* area?
]


---

# Obtaining the p-value and Deciding

* The red area, i.e. the p-value, equals **0.019** in our case.


```r
p_value &lt;- null_distribution %&gt;%
  get_p_value(obs_stat = obs_diff_prop, direction = "right")
p_value$p_value
```

```
## [1] 0.019
```

* In a world without discrimination, we would get `\(\hat{p_m} - \hat{p_f}\)` superior (or equal) to 29.2%p only 1.9% of the time. 

* So, we can reject `\(H_0\)`, i.e. the absence of discrimination, at the 5% significance level. 

  * We also say that `\(\hat{p_m} - \hat{p_f} = 29.2\)`%p is **significantly different from 0** at the 5% level. 
 
--

* **Question**: Suppose we had set `\(\alpha = 0.01 = 1\%\)`, would we have rejected the absence of discrimination at this level? 

---

# Testing Errors

Working with probabilities implies that sometimes, we make **errors**.

* A 29%p difference may be *unlikely* under `\(H_0\)`, but that **doesn't mean it's *impossible* to occur**.

  * In fact, such a difference (or higher) would occur in 1.9% of the cases.

* So, it may happen that we sometimes reject `\(H_0\)`, when in fact it was true.

  * Setting 5% significance level, you make sure it won't happen more than 5% of the time. 

---

# Testing Errors

In fact, in hypothesis testing, there are even **two types of errors** to make! ðŸ˜²:

.pull-left[

![:scale 100%](../img/photos/gt_error_table_ht.png)

]

.pull-right[

* **Type I error**: We Reject a *true* Null.
  
* **Type II error**: We *fail* to reject a *wrong* Null.

]

* In practice, we choose the frequency of a Type I error by setting `\(\alpha\)` and try to minimize the type II error.

* This is similar to a verdict reach in a court trial:

![:scale 48%](../img/photos/gt_error_table.png)

---

# Teaser for next week 

* Now you have all the tools to make **statistical inference** for real! 

* The regression tables no longer hold any secrets for you:

  * (OLS) Coefficients estimates

  * (Adjusted) `\(R^2\)`

  * Test-statistics

  * p-value

--

* Well, we will see this next week!

---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| &lt;a href="mailto:florian.oswald@sciencespo.fr"&gt;.ScPored[&lt;i class="fa fa-paper-plane fa-fw"&gt;&lt;/i&gt;]               | florian.oswald@sciencespo.fr       |
| &lt;a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Slides |
| &lt;a href="https://scpoecon.github.io/ScPoEconometrics"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Book |
| &lt;a href="http://twitter.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-twitter fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                         |
| &lt;a href="http://github.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-github fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                       |

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../js/ru_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
