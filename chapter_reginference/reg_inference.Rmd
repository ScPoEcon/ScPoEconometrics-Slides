---
title: "ScPoEconometrics"
subtitle: "Regression Inference"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")


knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

overwrite = FALSE

```

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Recap from last week

</br>

* **Confidence Interval**: A pluasible range of value for the population parameter

* **Hypothesis testing** : Null hypothesis vs Alternative hypothesis, (observed) test statistic, Null distribution, p-value

--

## Today: 

.pull-left[

* Statistical Inference in the regression framework

  * Understand and a regression table. 

* Cmparing Theory based and Simulated based inference

]

--
.pull-right[

* Classical Regression Model assumptions 

* Applications

  * Class size and student achievments 
  * Returns to education across gender 
]
  
---

# Back to class size and student achievments
 
* Let's take back the STAR experiment data, and focus on: 

  * *small* and *regular* classes
  * *Kindegarten* grade

--

* We'll consider the following model:

$$ \textrm{math_score}_i = b_0 + b_1 \textrm{small}_i + e_i$$ 

--

* Running the regression our model's prediction for student $i$ math score is:

```{r echo = FALSE}
star_df = read.csv("https://www.dropbox.com/s/bf1fog8yasw3wjj/star_data.csv?dl=1")
star_df = star_df %>%filter(star != "regular+aide" & grade == "k") 
star_df$small = 1*(star_df$star == "small")+0
star_df = star_df[complete.cases(star_df),]

reg_star = lm(math~small, star_df)
coeff_star = round(reg_star$coefficients,1)
```

$$ \hat{\textrm{math_score}_i} = `r coeff_star[1]` + `r coeff_star[2]` * \textrm{small}_i$$

--

* So $b_1 = `r coeff_star[2]`$. What if we would draw a new random sample of schools<sup>1</sup> from Tennessee and redo the experiment, would we find a different value for $b_1$?


.footnote[
[1]: For simplicity we assume that schools which participated in the STAR exeriment were randomly drawn.]

--

* We know the answer is *yes*, but how much different this estimate would likely to be? 

---


# Regression inference: $b_k$ vs $\beta_k$ 

* $b_0, b_1$ are the **point estimates** computed from our sample.
  
  * Like the sample proportion $\hat{p}$ was in our pasta example!

--

.pull-left[

* In fact, our model's prediction...
    $$\hat{y} = b_0 + b_1 x_1$$
]

--

.pull-right[

... is an **estimate** about an unknown, **true population line**
$$y = \beta_0 + \beta_1 x_1$$
]

where $\beta_0, \beta_1$ are the **population parameters** of interest.

--

* You will often find $\hat{\beta_k}$ rather than $b_k$, both refer to sample estimate of $\beta_k$

--

* Let's bring what we know about Confidence Intervals, Hypothesis tests and Standard Errors to bear on those $\hat{\beta_k}$!

---

# Regression Table

Here is the R `summary` of our regression:
```{r}
summary(lm(math~small, star_df))
```

---

# Understanding Regression Tables

* Let's focus on:

```{r echo=FALSE}
reg_summary_star_df = round(summary(reg_star)$coefficients,3)
reg_summary_star_df
```


* There are 3 new columns here: `std_error`, `t value`, `Pr(>`&#8739;`t`&#8739;`)`.

--

Entry | Meaning
----- | ----
`Std. Error` |  Standard Error of $b_k$
`t value` |  Observed Test statistic associated to $H_0:\beta_k = 0,H_A:\beta_k \neq 0$
`Pr(>`&#8739;`t`&#8739;`)` |  p-value associated to $H_0:\beta_k = 0,H_A:\beta_k \neq 0$

--

* So, nothing really new actually.

--

* Let's focus on the `small` coefficient and make sense of each entry in the case of a regression. 

---

# Standard Error of $b_k$

> *Standard Error of $b_k$* $\equiv$ Standard deviation of the sampling distribution of $b_k$.

Let's imagine we could redo the experiment 1000 times on 1000 different samples 

* We'd run 1000 regression and would get 1000 estimates for each $b_k$.

* The standard error of $b_k$ quantifies how much variation in $b_k$ one would expect across (*an infinity of*) samples.
  
---

# Standard Error of $b_\textrm{small}$

* From the table, we get $\hat{\textrm{SE}}(b_\textrm{small}) = `r reg_summary_star_df[2,2]`$
  
  * Notice that we write $\hat{\textrm{SE}}$ and not ${\textrm{SE}}$ because `r reg_summary_star_df[2,2]` is the estimate of the real standard error of $b_\textrm{small}$ we get from our sample

--

* Let's simulate the sampling distribution of $b_\textrm{small}$ to see where it comes from.

```{r echo=FALSE}
library(infer)
bootstrap_distrib = star_df %>% 
    specify(formula = math~small) %>%
    generate(reps = 1000, type = "bootstrap") %>%
    calculate(stat = "slope")
se_simul = round(sd(bootstrap_distrib$stat),3)
```
    
---

class:inverse

# Task 1 (10 min)

As we did for the sampling distribution of the proportion of *green pasta*, we want to generate the bootstrap distribution of $b_\textrm{small}$. 

1. Download the data from [here]("https://www.dropbox.com/s/bf1fog8yasw3wjj/star_data.csv?dl=1") and assign it to `star_df`

--

1. Filter the dataframe to keep only:
  * *small* and *regular* classes
  * *Kindegarten* grade

--

1. Create a dummy variable named `small` equal to one if `star` is equal to *small*

--

1. Keep only the rows without any NA.

--

1. Compute the bootstrap distribution of $b_\textrm{small}$ based on 1000 samples drawn `star_df`. *Hint*:  Use the appropriate functions from the `infer` package. 

--

1. Plot this simulated sampling distribution and compute the standard error of $b_\textrm{small}$ 

---

# Testing $\beta_k = 0$ vs $\beta_k \neq 0$

By default, the regression output provides the results associated to following hypothesis test: 

$$\begin{align}H_0:& \beta_k = 0\\H_A:& \beta_k \neq 0\end{align}$$

* It allows to statistically test if there is a true relationship between the outcome and our regressor. 
--

* If $H_0$ is true, there is **no** relationship between outcome and our regressor. 

  * In that case observing $b_1 \neq 0$ was just chance.

--

* If $H_0$ is false, then there **is** a true relationship. 

---

# Test statistic and p-value

* As we saw in the previous lecture, to conduct such a test we will:

--

  * Derive the sampling distribution of our **test statistic** (`t value`) assuming $H_0$ is true, i.e. the *Null distribution*.
  
--

  * Quantify how extreme the **observed test statistic** is in this hypothetic world.

--

* Our *observed test statistic* (`t value`) equals $\frac{b}{\hat{SE}(b)}$.

  * Why not just taking $b$? We'll come back and explain this formula later.

* The **p-value** (measures the area outside of $\pm$ *observed test statistic* under the *Null distribution*.

* Finally, we will see if we can reject $H_0$ at the usual **significance levels**: $\alpha$ = 0.1, 0.05, 0.01. (`Signif. codes` in the R summary of the regression)

---

# Testing $\beta_\textrm{small} = 0$ vs $\beta_\textrm{small} \neq 0$

* We will approximate the Null distribution of $\frac{b_\textrm{small}}{\hat{SE}(b_\textrm{small})}$ through a simulation exercise. 

--

* If there is no relationship between the math score and class size ($\H_0$ is true), then *reshuffling* / *permuting* the values of `small` across students should play no role.

--

.pull-left[
* Let's generate 1000 permuted samples and compute $b_\textrm{small}$ for each.

```{r}
null_distribution <- star_df %>% 
  specify(math ~ small) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "slope")
```
]

--

.pull-right[

* We can compute the distribution of our test statistic $\frac{b_\textrm{small}}{\hat{SE}(b_\textrm{small})}$ under the Null:

```{r}
null_distribution$test_stat = null_distribution$stat/sd(bootstrap_distrib$stat)
```

* Remember we got $\hat{SE}(b_\textrm{small})$ = `r round(sd(bootstrap_distrib$stat),3)` from our bootstrap distribution.
]

  


---

# Testing $\beta_\textrm{small} = 0$ vs $\beta_\textrm{small} \neq 0$


```{r echo=FALSE, fig.height=5}
null_distribution %>%
  ggplot(aes(x = test_stat)) +
  geom_histogram(col = "white", fill = "darkgreen") +
  labs(x = "Test Statistic under the Null",
       y = "Frequency",
       title = "Simulated based Null Distribution") +
  theme_bw(base_size = 14)+
  xlim(-3.5,3.5)

```


---

# Testing $\beta_\textrm{small} = 0$ vs $\beta_\textrm{small} \neq 0$

* The *observed test statistic* is 

```{r}
(observed_stat = reg_star$coefficients[2]/sd(bootstrap_distrib$stat))
```

* Quite close to the observed test statistic we get from the table: `t value` = `r reg_summary_star_df[2,3]`.


--

.pull-left[
```{r echo=FALSE, fig.height=4.25}
plot_null_distrib = null_distribution %>%
ggplot(aes(x = test_stat)) +
  geom_histogram(col = "white", fill = "darkgreen") +
  geom_vline(xintercept = observed_stat, size =2) +
  labs(x = "Test Statistic under the Null",
       y = "Frequency",
       title = "Simulated based") +
  theme_bw(base_size = 20)+
  xlim(-6,6)
plot_null_distrib
```

]

--

.pull-right[

</br> 

* Our observed test statistic is far away from the Null distribution.

* It means it is very unlikely to get $b_\textrm{small}$ = `r coeff_star[2]` when $H_0$ is true.

]

---

# Testing $\beta_\textrm{small} = 0$ vs $\beta_\textrm{small} \neq 0$

* To decide if we reject $H_0$, remember we are considering a **two-sided test** here, i.e. *more extreme* means inferior to - `r round(observed_stat,3)` **or** superior to `r round(observed_stat,3)`. 


--


.pull-left[

* Computing the *p-value* we get 0, as in the table. 

```{r}
(p_value = mean(null_distribution$test_stat < -observed_stat | 
                  null_distribution$test_stat > observed_stat))
```

* Graphically, it corresponds to the area outside the two black lines.

  * We clearly see there is no observation that fall in this area. 
]


.pull-right[
```{r echo = FALSE, fig.height=5.5}
plot_null_distrib +
    geom_vline(xintercept = -observed_stat, size =2) +
  xlim(-7,7)
  
```

]

---

# Testing $\beta_\textrm{small} = 0$ vs $\beta_\textrm{small} \neq 0$


* Since the *p-value* is equal to 0 it means that we would reject $H_0$ at any significance level.

  * The p-value would always be inferior to $\alpha$.

* In other words, we can se that $b_\textrm{small}$ is **statistically different from 0** at any significance level.

  * We also say that $b_\textrm{small}$ is *statistically significant* (at any significance level).


---

layout: false
class: title-slide-section-red, middle

# Regression Inference: Theory

---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Regression Inference: Theory

* Up to now we presented simulation based inference. 

* The values reported by statistical packages like R are instead obtained from theory. 

* Theoretical inference is based on **large sample approximations**.
  * One can show that the sampling distributions converge to suitable distributions.
  
* Let's have a brief overview of theory based approach. 

---

# Regression inference: Theory

* Theory-based approach uses one fundamental result: 

$$ \frac{b - \beta}{\hat{\textrm{SE}(b)}} \underset{n \rightarrow \infty}{\longrightarrow} \mathcal{N}(0,1)$$

--

* It states that the sample statistic $\frac{b - \beta}{\hat{\textrm{SE}(b)}}$ converges to a *standard normal distribution* as $n$ tends to infinity. 

  * $\hat{\textrm{SE}(b)}$ is the sample estimate of the standard deviation of $b$
  * It is also obtained through a theoretical formula (which you can find in the [book](https://scpoecon.github.io/ScPoEconometrics/std-errors.html#se-theory)!)

--

* We don't need to simulate any sampling distribution here, we can derive it from theory and use it to construct confidence interval or to conduct tests. 

---

# Theory based Inference: Confidence Interval

* Let's take the example of a 95% Confidence Interval.

--

* Because the sampling distribution of $b$ is normally shaped, we can use the ***95% rule of thumb*** about normal distributions. 

--

* We know indeed that 95% of the values of a normal distribution lie within 1.96 (~2) standard deviations of the mean.

--

* So, we can derive that a 95% CI for $\beta$ is: 

$$\textrm{CI}_{95\%} = [ b \pm 1.96*\hat{\textrm{SE}}(b)]$$

--

* It easily generalizes to any confidence level by taking the appropriate quantile of the normal distribution. 

---

# Theory based Inference: Hypothesis testing

* As we already mentioned, the default test that is conducted by any statistical software is:

$$\begin{align}H_0:& \beta_k = 0\\H_A:& \beta_k \neq 0\end{align}$$
--

* So, **under the Null hypothesis** we get from the previous formula that:

$$ \frac{b}{\hat{\textrm{SE}(b)}} \underset{n \rightarrow \infty}{\longrightarrow} \mathcal{N}(0,1)$$ 

--

* Then we can directly compare the observed value of $\frac{b}{\hat{\textrm{SE}(b)}}$ to the *standard normal distribution* which is **Null distribution** of our test statistic. 

--

* The **p-value** assoiated to our test is then equal to the area of the *standard normal distribution* outside $\pm$ the observed value of $\frac{b}{\hat{\textrm{SE}(b)}}$.

---


# Classical Regression Model

* Whether the inference is made from theory of simulations, some assumptions have to be met for this inference to be valid.

* The set of assumptions needed defines the *Classical Regression Model* (CRM)

--

* Before looking at these assumptions, let's see the small but important modifications we apply to our model (back to [*lecture 4*](https://raw.githack.com/ScPoEcon/ScPoEconometrics-Slides/master/chapter3/chapter3.html#1)):

  * We already mentioned the distinction between the sample estimate $b_k$ (or $\hat{\beta_k}$) and the population parameter $\beta_k$.
  
  * In the same way, we distinguish $e$, the sample error, from $\varepsilon$ the error term from the true population model: 
  
  $$y_i = \beta_0 + \beta_1 x_{1,i} + ... + \beta_k x_{k,i} + \varepsilon_i$$
  
---


# CRM Assumptions

1. The data are **not linearly dependent**: Each variable provides new information for the outcome, and it cannot be replicated as a linear combination of other variables.

--

2. The mean of the residuals conditional on $x$ should be zero, $E[\varepsilon|x] = 0$. Notice that this also means that $Cov(\varepsilon,x) = 0$, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*.

--

3. The data are drawn from a **random sample** of size $n$: observation $(x_i,y_i)$ comes from the exact same distribution, and is independent of observation $(x_j,y_j)$, for all $i\neq j$.

--

4. The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.

--

5. The error is normally distributed, i.e. $\varepsilon \sim \mathcal{N}(0,\sigma^2)$ 
  * This last assumption allows to avoid large sample apprximations, but it is never used in practice since samples are sufficiently large $(n \ge 30)$.

---

# Comparing Simulation and Theory based inference

* Theory and simulated based approaches often yields very similar results.

--

* Coming back to $b_\textrm{small}$, here is a recap of what we found:

Estimate | Theory |  Simulation 
--------- | ------ | --------
Std. Error   | `r reg_summary_star_df[2,2]` |  `r se_simul`
Observed test stat.| `r reg_summary_star_df[2,3]` | `r round(observed_stat,3)`
p-value | `r reg_summary_star_df[2,4]` | `r p_value`

--


* Let's have a look at Confidence Intervals from both approaches. 

---

# Comparing Confidence Intervals 

* From theory we have: $\textrm{CI}_{95\%} = [8.9 \pm 1.96*1.678]$
```{r}
lower_bound_theory = 8.9 - 1.96*1.678
upper_bound_theory = 8.9 + 1.96*1.678
(CI_theory = c(lower_bound_theory, upper_bound_theory))
```

--

* From simulation (*percentile method*): 
```{r}
lower_bound_simul = quantile(bootstrap_distrib$stat,0.025)
upper_bound_simul = quantile(bootstrap_distrib$stat,0.975)
(CI_simul = c(lower_bound_simul, upper_bound_simul))
```

--

* Again, both appraoches yields very similar results. 

--

* We used simulation based inference to give you the intuition behind statistical inference.

---

# Comparing Confidence Intervals

```{r, echo=FALSE,  fig.height=5, fig.width=8}
visualize(bootstrap_distrib, fill = "darkgreen") + 
  shade_confidence_interval(endpoints = CI_theory, fill = NULL, 
                            linetype = "solid", color = "blue", size =1) + 
  shade_confidence_interval(endpoints = CI_simul, fill = NULL, 
                            linetype = "dashed", color = "black", size =1) + 
  theme_bw(base_size = 14) +
  labs(x = "b_small", y = "Frequency", title = "Theory based (solid blue) vs Simulated based (dotted black) 95% CIs") +
  xlim(lower_bound_theory-2,upper_bound_theory+2)

```


---

class:inverse

# Task 2.1 (10 min)

Let's go back to our question of returns to education and gender. 

1. Load the data `CPS1985` from the `AER` package and look back at the `help` to get the definition of each variable: `?CPS1985`

1. Create the`log_wage` variable equal to the log of `wage`.

--

1. Regress `log_wage` on `gender` and `education`, and save it as `reg1`. 

  3.1 Interpret each coefficient.
  
  3.2 Are the coefficients statistically significant? At which significance level? 
  
--

1. Regress the `log_wage` on `gender`, `education` and their interaction `gender*education`, save it as `reg2`. 

  4.1 How do you interpret the coefficient associated to $female*education$
  
  4.2 Can we reject the nullity of this coefficient at the 5% level? At 10%?  
  
---

class:inverse

# Task 2.2 (10 min)

$5.$ Produce a scatterplot of the relationship between the log wage and the level of education.

--

$6.$ Add the *regression line* with `geom_smooth`. What does this line represents?

--

$7.$ Let's illustrate what the shaded area stands for.

  $\quad 7.1$ Draw one bootstrap sample from our `cps` data. 

  $\quad 7.2$ Regress the `log_wage` on `gender`, `education` and their interaction `gender*education`, save it as `reg_bootstrap`.

  $\quad 7.3$ From `reg_bootstrap` extract and save the value of the intercept for men as `intercept_men_bootstrap` and the value of the slope for men as `slope_men_bootstrap`. Do the same for women.

  $\quad 7.4$ Add both predicted lines from this bootstrap sample to the previous plot (*Hint*: use `geom_abline` (x2))
  
---


# Illustrating Uncertainty

.pull-left[
![](../img/gifs/wages.gif)

]

.pull-right[
</br>
* [`ungeviz`](https://github.com/wilkelab/ungeviz) and `gganimate` bring you: Moving Lines!

* We took 20 bootstrap samples from our data

* You can see how different data points are included in each bootstrap sample.

* Those different points imply different regression lines.

* You should remember those moving lines when looking at the shaded area!

]

---

# Teaser for the Next (last!) session


---



class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

Big Thanks `r emo::ji("tada")` to [ungeviz](https://github.com/wilkelab/ungeviz) and `r emo::ji("confetti")` [gganimate](https://github.com/thomasp85/gganimate) for their awesome packages!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

