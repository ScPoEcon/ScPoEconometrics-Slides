---
title: "ScPoEconometrics"
subtitle: "Multiple Linear Regression Model"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---


layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)
library(plotly)
library(reshape2)
library(haven)
library(tidyverse)
library(AER)
library(gtools)
```


# Recap from last week

* Causality versus correlation

* The Potential Outcome Framework a.k.a. Rubin's Causal Model

* Randomized controlled trials (RCTs)

--

## Today - Multiple Linear Regression Model

* **Multiple Linear Regression Model**

* *Extensions:* standardized regression; log models; interacting variables

* Empirical applications:

  * *Class size* and *student performance*
  
  * *Wage*, *education* and *gender* 

---

# Class size and student performance

* Let's go back to Angrist and Lavy's (1999)'s analysis of the effect of class size on student performance in Israel.

--

* With a **simple linear regression**, we found that class size was positively ***associated*** with students' scores in maths and reading.

---

# Class size and student performance: Raw relationship

```{r, echo=FALSE, fig.height=4.75, fig.width = 8}
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")

set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 14)
```

---

# Class size and student performance: Raw relationship

```{r, echo=FALSE, fig.height=4.75, fig.width = 8}
set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 14) +
  geom_smooth(se = FALSE, method = "lm")
```

---

# Class size and student performance: Raw relationship

.pull-left[
```{r, echo=FALSE, fig.height=6}
set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```
]

.pull-right[

```{r}
lm(avgmath ~ classize, grades)
```

* *Interpretation:* ***On average***, a 1-student increase in class size is ***associated*** to a `r round(lm(avgmath~classize, grades)$coefficients[2],2)` points increase in average math score.
]

---

# Class size and student performance

* Let's go back to Angrist and Lavy's (1999)'s analysis of the effect of class size on student performance in Israel.

* With a **simple linear regression**, we found that class size was positively ***associated*** with students' scores in maths and reading.

* This is intuitively unexpected, and contrasts with the simple results from the *STAR* randomized experiment.

--

* Could it be that some other variable may be related to class size ***as well as*** students' performance?

* In particular, we mentioned the **location effect**: large classes may be more common in wealthier and bigger cities, while small classes may be more likely in poorer rural areas.

--

* Let's investigate this hypothesis.

---

# Class size and student performance: Confounders

.pull-left[
Link between **class size** and **the share of student who come from `disadvantaged` background** in the class.

```{r, echo = FALSE, fig.height=4.75, fig.width = 8}
ggplot(grades, aes(x = disadvantaged, y = classize)) + 
  geom_point(size = 2, alpha = 0.4, position = "jitter") +
  xlim(0,100) +
  ylim(0,45) +
  labs(
    x = "% class coming from disadvantaged background",
    y = "Class size",
    title = "Relationship with class size") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```

`r emo::ji("point_right")` On average, there is a greater % of disadvantaged students in smaller classes.
]

--

.pull-right[
Link between **average math score** and **the share of student who come from `disadvantaged` background** in the class.

```{r, echo = FALSE, fig.height=4.75, fig.width = 8}
ggplot(grades, aes(x = disadvantaged, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.4, position = "jitter") +
  xlim(0,100) +
  ylim(0,100) +
  labs(
    x = "% class coming from disadvantaged background",
    y = "Average math score",
    title = "Relationship with average math score") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```

`r emo::ji("point_right")` On average, the greater the % of students coming from a disadvantaged background, the lower the average math score.

]
---

# Class size and student performance: Multiple regression

* Suppose we want to know the effect of class size on average math scores, ***controlling for*** the fact that there is a negative relationship between the % of disadvantaged students and class size **AND** average math score.

--

* To do so, we have to include both `classize` and `disadvantaged` variables as *regressors* in the regression.

--

* As such we can obtain an estimate of the effect of class size on average math score, ** *purged* of the effect of the disadvantaged variable**.

--

* The model we want to estimate becomes:

$$
 \textrm{mathscore}_i = b_0 + b_1 \textrm{classize}_i + b_2 \textrm{disadvantaged}_i + e_i
$$
--

* This is ***multiple regression***! We will estimate this model in a few slides. Let's formalize what we have seen so far.

---

# Multiple Regression's Purpose

* Recall from two weeks ago, the **Simple Linear Model** can be written as

$$y_i = b_0 + b_1 x_i + e_i,$$

where $y_i$ is the ***dependent variable*** and $x_i$ is the ***independent variable***.

--

`r emo::ji("warning")` Unless all other factors affecting $y_i$, i.e. $e_i$, are uncorrelated with $x_i$, $b_1$ **cannot be interpreted as a causal effect**.

--

We need to **enrich the model** and take into account factors that are simultaneously related to $y_i$ **and** $x_i$.

---

# Mutiple Regression Model

The expanded model can be written as:

$$y_i = b_0 + b_1 x_{1,i} + b_2 x_{2,i}  + b_3 x_{3,i} + \dots + b_k x_{k,i} + e_i,$$

where $x_1$, $x_2$, ..., $x_k$ are $k$ regressors, and $b_1$, $b_2$, ..., $b_k$ are the associated $k$ coefficients.

--

***Estimation***: We obtain the values for $(b_0, b_1, b_2, ..., b_k)$ in the same way as before, using **OLS**. 

* $(b_0^{OLS}, b_1^{OLS}, b_2^{OLS}, ..., b_k^{OLS})$ are the values that minimize the **Sum of Squared Residuals**. 
  
* That is they minimize
$$
\begin{align}
\sum_{i}{e_i^2} &= \sum_{i}{(y_i - \hat{y_i})^2} \\
&= \sum_{i}{[y_i - (b_0 + b_1 x_{1,i} + b_2 x_{2,i}  + b_3 x_{3,i} + \dots + b_k x_{k,i})]^2}
\end{align}
$$

---

# Multiple Regression Model: Interpretation

For now assume both the dependent variable $(y_i)$ and the independent variables $(x_k)$ are numeric.

> Intercept $(b_0)$: **The predicted value of $y$ $(\widehat{y})$ if all the regressors $(x_1, x_2, x_3,...)$ are equal to 0.**

--

> Slope $(b_k)$: **The predicted change, on average, in the value of $y$ *associated* to a one-unit increase in $x_k$...** <br/>
> $\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad$ **... keeping all the other regressors constant!**

--

* Notice that the *keeping all the other regressors constant* is the only part that changes compared to SLM.

* In other words, you are considering the indidivudal effect of the variable $x_k$ on $y$ **in isolation** of the effect that the other regressors might have on $y$.

--

* **Link with causal inference**: Only the regressors included in the model are held constant, those that are not in the model can still vary and "bias" your estimates.

---

# Multiple Regression with `R`

* Very similar to simple linear regression:

```{r, echo = TRUE, eval = FALSE}
lm(formula = dependent variable ~  independent variable 1 + independent variable 2 + ...,
   data = data.frame containing the data)
```

--

## Class size and student performance: Multiple regression

Let's estimate the model from earlier on by OLS: $\textrm{mathscore}_i = b_0 + b_1 \textrm{classize}_i + b_2 \textrm{disadvantaged}_i + e_i$
 
```{r}
lm(avgmath ~ classize + disadvantaged, grades)
```


---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Questions***

1. How do you interpret each of these coefficients?
1. How do you explain the change in the `classize` coefficient compared to the SLM case?

---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Answers***

1\. How do you interpret each of these coefficients?

* $b_0$ = 69.9: When `class size` and `disadvantaged` are set to 0, the *predicted* value of the average math score is 69.9.

--

* $b_1$ = 0.07: Keeping the share of *disadvantaged students* constant in the class, a 1-student increase in class size is ***associated, on average,*** with a 0.07 point increase in average math score.

--

* $b_2$ = - 0.34: Keeping the *class size* constant, a 1-percentage point increase in the share of *disadvantaged students* is ***associated, on average,*** with a 0.34 point decrease in average math score.

---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Answers***

2\. How do you explain the change in the `classize` coefficient compared to the SLM case?

* $b_1$ decreases when the `disadvantaged` variable is taken into account. This was expected since part of the positive effect of class size was partly due to the smaller share of disadvantaged students in bigger classes.
  
---

# Multiple Regression in 3D


```{r plane3D-reg, echo=FALSE, message=FALSE, warning=FALSE}
# linear fit
fit <- lm(avgmath ~ classize + disadvantaged, grades)
 
to_plot_x <- range(grades$classize)
to_plot_y <- range(grades$disadvantaged)

df <- data.frame(classize = rep(to_plot_x, 2),
           disadvantaged = rep(to_plot_y, each = 2))

df["pred"] <- predict.lm(fit, df, se.fit = F)

surf <- acast(df,disadvantaged ~ classize)

color <- rep(0, length(df))
grades %>%
  plot_ly(width = 5, height = 5, tickfont = list(size =7)) %>%
  add_markers(x = ~classize, y = ~disadvantaged, z = ~avgmath,
              name = "1 dot = 1 class",opacity = .6, 
              marker=list(color = 'blue',
                          size = 1.5, 
                          hoverinfo="skip")) %>%
  add_surface(x = to_plot_x, y = to_plot_y, z = ~surf,
              inherit = F,
              name = "3D regression plane",
              opacity = .7, cauto = F,
              colorscale = list(c(0, 1), c("red", "yellow"))) %>%
   layout(xaxis = list(tickfont = list(size =7)), yaxis = list(tickfont = list(size =7)))
```


---

class:inverse

# Task 1 (10 minutes)

Let's analyse the regression results using **reading** score as the dependent variable.

1. Load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1) using the `read_dta()` function from the `haven` package. Assign it to an object `grades`.

1. Regress `avgverb` on `classize` and `disadvantaged`. Assign the output to a new object `reg`. 

1. Look at the coefficients of this regression by running `reg$coefficients`. How do they compare with the math score regression coefficients?

1. What are the other available variables that we may add in the regression? 
  * Run the regression with all these variables and assign it to `reg_full`.
  * Look at the coefficients.
  * Discuss all coefficients: sign and magnitude.

---

# No Perfect Collinearity

There is one condition to satisfy to add regressors to the model:

> Any additional variable needs to add **at least *some* new information**.

--

In other words, regressors **cannot be perfectly collinear**, i.e. not linear combinations of one another:

  $$ x_2 \neq ax_1 + b $$ 

--

Even if not perfectly correlated, the individual effects of highly correlated regressors are hard to disantangle.

--

Note that this implies that the number of observations has to be greater than the number of independent variables.

---

class: inverse

# Task 2 (7) minutes)

Still need to think about this perfect collinearity issue? Let's run a regression where there is perfect linear dependence between regressors.

1. Load the *STAR* data from [here]("https://www.dropbox.com/s/bf1fog8yasw3wjj/star_data.csv?dl=1") and assign it to an object called `star_df`. Keep only cases with no `NA`s with the following code:  
`star_df <- star_df[complete.cases(star_df),]`  
Keep only second graders and small and regular class groups.

1. Create a dummy variable `small` equal to `TRUE` if students are in a small class and `FALSE` if they are in a regular class. In the same way, create a dummy variable `regular` equal to `TRUE` if students are in a regular class and to `FALSE` if they are in a small class. Clearly these two variables are perfectly collinear.

1. Regress `small` on `math`. What is the average predicted `math` score of students in a small class?

1. Regress `regular` on `math`. What is the average predicted `math` score of students in a regular class?

1. Regress `small` and `regular` on `math`. What do you notice? Can you explain why?

---

# Adjusted $R^2$

* Not of great importance but because it is so widely reported you just need to know it.

--

* By construction, $R^2$ will always increase when a new regressor is added to the regression.

--

* The *adjusted $R^2$* imposes a penalty for adding regressors to the model. The details are not crucial as in the vast majority of cases the $R^2$ and the adjusted $R^2$ are pretty similar.

---

# Ommitted variable bias (OVB)

* ***ommitted variable bias***: Omitting important control variables from the regression model

* This renders the coefficient for your regressor of interest unreliable.

--

* Recall our previous example of class size and disadvantaged students. Not taking into account the `disadvantaged` variable let to a OVB for our class size estimate.

--

* The formula for OVB is:

$$\textrm{OVB} = \{ \textrm{Relationship between } disadvantaged_i \textrm{ and } classize_i \} \\ * \{ \textrm{Effect of } disadvantaged_i \textrm{ in multiple regression} \}$$

--

* From this formula you obtain both the *mangitude* of OVB and its *sign* (positive/negative).

--

* The relationship between `disadvantaged` and `classize` is negative, and the effect of `disadvantaged` in the multiple regression is negative, therefore the OVB is positive. In other words, ommitting `disadvantaged` from the regression leads to an inflated coefficient for class size.

---

# Variations from the baseline model 

Depending on the dataset and the relationships between the variables of interest, you may need to move away from the baseline model.  

* We will focus on 3 important variations:

  * ***Standardized regressions***

  * ***Log models***

  * ***Interactions between regressors***

--

* In each case, the way we estimate these coefficients does not change (i.e OLS).

--

* However, the way we interpret our coefficients $(b_1, b_2,..., b_k)$ does change!

---

# Standardized Regression

Let's define what *standardizing* a variable means.

> ***Standardizing*** a variable $z$  means to *demean* the variable and to divide the demeaned value by its own standard deviation:

$$ z_i^{stand} = \frac{z_i - \bar z}{\sigma(z)}$$ 
where $\bar z$ is the mean of $z$ and $\sigma(z)$ is the standard deviation of $z$, i.e. $\sigma(z) = \sqrt{\textrm{VAR}(z)}$.

* Why would we do that in the first place?

--

* Intuitively, standardizing **puts variables on the same scale** so we can compare them.
  
* In our class size and student performance example, it will help to interpret: 

  * The **magnitude** of the effects,
  * The **relative importance of each variable**.

---

# Standardizing Regression: Interpretation

* If the dependent variable $y$ is standardized:

--

  * By definition, $b_k$ measures the predicted change in ** $y^{stand}$ **  associated with a one unit increase in $x_k$.
  
  * If $y^{stand}$ increases by one, it means that $y$ increases by one standard deviation. So $b_k$ measures the change in $y$ **as a share of $y$'s standard deviation**.
  
--

* If the regressor $x_k$ is standardized:

--

  * By definition, $b_k$ measures the predicted change in $y$ associated with a one unit increase in ** $x_k^{stand}$ **. 
  
  * If $x_k^{stand}$ increases by one unit, it means that $x_k$ increases by one standard deviation. So $b_k$ measures the predicted change in $y$ **associated with an increase in $x_k$ by one standard deviation**. 
  
---

class:inverse

# Task 3 (7 minutes)

Let's go back our `grades` dataset. These are the estimates we got from regressing the math test score on the full set of regressors.

```{r echo = FALSE}
reg_full <- lm(avgverb ~ classize + disadvantaged + school_enrollment + female + religious, grades)
reg_full$coefficients
```

1. Create a new variable `avgmath_stand` equal to the the standardized math score. You can use the `standardize()` function from the `jtools` package or do it by hand with base `R`.

1. Run the full regression using the standardized math test score as the dependent variable. Interpret the coefficients and their magnitude.

1. If you were to pick the most influential variable on the math score, what would it be?

1. Add the standardized variables for each *continuous* regressor as `<regressor>_stand`.
  * Would it make sense to standardize the `religious` variable?

1. Regress `avgmath_stand` on the full set of standardized regressors and `religious`. Discuss the relative influence of the regressors.
---

# Log Models

* The models we have seen so far can be called ***level-level*** specifications. Both the dependent and the independent variables have been measured in level.

--

  * This *level* can be: euros, years, number of students,... and even percentage.
  
--

* Taking the log of the dependent and/or the independent variable(s) leads us to define 3 other types of regressions:

  * ***Log - level***: $\quad log(y_i) = b_0 + b_1 x_{1,i} + ... + e_i$

  * ***Level - log***: $\quad \textrm{y}_i = b_0 + b_1 log(x_{1,i}) + ... + e_i$

  * ***Log - log***: $\quad log(y_i) = b_0 + b_1 log(x_{1,i}) + ... + e_i$

---

# Log Models: Interpretation

Here is a table that summarise how to interpret regressor coefficients in each case:  

|    Model           | Equation  |  Interpretation of $b_1$            |
|--------------------|:---------:|:-----------------------------------:|
| Level - Level | $y = b_0 + b_1 x_{1} + e$ | .small[**One unit** increase in ] $x_1$ .small[ is associated,</br> on average, with ] $b_1$ .small[**unit change** in y]  |
| Log - Level | $log(y) = b_0 + b_1 x_{1} + e$ | .small[**One unit** increase in ] $x_1$ .small[ is associated,</br> on average, with] $b_1$ .small[ **percent change** in y]  |
| Level - Log | $y = b_0 + b_1 log(x_{1})  + e$ | .small[**One percent** increase in ] $x_1$ .small[ is associated,</br> on average, with] $b_1/100$ .small[**unit change** in y] |
| Log - Log  |  $log(y) = b_0 + b_1 log(x_{1}) + e$ | .small[**One percent** increase in ] $x_1$ .small[ is associated,</br> on average, with] $b_1$ .small[**percent change** in y]  |


* It looks like cooking recipes but of course it can be derived with some calculus. 

---

# When do we use log models?

There are several reasons why we would want to use log models:

--

* Account for ***non linearity*** in the relationship between $y$ and $x$.

--

* Interpret coefficients as <a href="https://en.wikipedia.org/wiki/Elasticity_(economics)">***elasticities***</a> which play a central role in economic theory.

--

* Limit the influence of ***outliers*** (due to the concavity of the *log* function).

---

# Interacting regressors

* We interact two regressors when we think the effect of one depends on the value of the other. 

  * In our next application, we'll let the return to education on wage vary by gender.
  
* In practice, if we interact $x_1$ and $x_2$, we would write our model like this : 

$$y_i =  b_0 + b_1 x_{i,1} + b_2 x_{i,2} + b_{1,2}x_{i,1}*x_{i,2} + ... + e$$

* The interpretation of $b_1$, $b_2$, and $b_{1,2}$ will depend on the type of $x_1$ and $x_2$. 
  
* Let's focus on the cases where one regressor is a dummy / categorical variable and the other is continuous.

* It will give you the intuition for the other cases : 

  * Both regresors are dummies / categorical variables

  * Both regresors are continous
  
---

# Interacting regressors

* Let's illustrate with the case of **doctors visits**. 
  *  .small[We use the `DoctorVisits` dataset from the `AER` package contains [cross-section data](https://en.wikipedia.org/wiki/Cross-sectional_data) originating from the 1977â€“1978 Australian Health Survey]

* In particular we will focus on the role of `gender` and `age` on the number of doctor `visits` in the past two weeks. 

* Our regression model is 

$$ \textrm{visits_i} = b_0 + b_1 \textrm{is.female}_i + b_2 \textrm{age}_i + b_3 \textrm{is.female}_i * \textrm{age}_i + e$$ 

* What is the predicted number of visits for 

  * A woman of age 30: $\hat{\textrm{visits}_i} = b_0 + b_1 + b_2*30 + b_3 * 30$
  * A man of age 30: $\hat{\textrm{visits}_{i'}} = b_0 + b_2*30$
  * The difference in the prediction between a man and woman of age 30 is then equal to: $b_1 + b_3*30$

---

# Interacting regressors

Running the regression we obtain 

```{r, echo = FALSE, eval = TRUE}
library(jtools)
data("DoctorVisits")
df = DoctorVisits
df$age = df$age*100
df$is.female = 1*(df$gender=="female")
round(lm(visits~ is.female + age + is.female*age, df)$coefficients,3)
```


* **Interpretation**

--

  * Thanks to our interaction term, we let the difference in the number of visits between men and women vary with age.
  
--
  
  * The coefficient assocciated to `is.female` is equal to 0.179 which means that we observe more doctor visits among *relatively young women.* 

--

  * The coefficient assocciated to `is.female:age` being negative it means this differential will *decrease with age*. 
  
---

# Interacting regressors

Here is a visualization of our model prediction

```{r,echo=F, fig.height=4, fig.width=7}
ggplot(df, aes(x = age, y = visits, group = gender, color = gender)) +
    geom_smooth(method = "lm", se = F) +
    theme_bw() +
  labs(title = "Number of visits in the past 2 weeks")
```


---

# Wages, education and gender

* Let's use these new *tools* to investigate the relationship between wages, education and gender. 

* We will use data from the [**C**urrent **P**opulation **S**urvey](https://www.census.gov/programs-surveys/cps.html). 
  * .small[This is the U.S. Government's monthly survey of unemployment and labor force participation.] 
  * .small[We'll use a sample of 534 individuals from the 1985 CPS available in the `AER` package.]

--

* Let's first have a look at the distribution of wage broken down by education and gender 

.pull-left[
```{r, echo = FALSE, fig.height=2.5,fig.width=3.5}
data("CPS1985")
cps = CPS1985
cps$education_gp = quantcut(cps$education,3)
cps$log_wage = log(cps$wage)
  
ggplot(cps, aes(x = gender, y = wage, fill = gender)) + 
    geom_boxplot() + 
    theme_bw() + 
    labs(title = "Boxplot: Wage ~ gender")+
    guides(fill=FALSE)
```
]

.pull-right[
```{r, echo = FALSE, fig.height=2.5,fig.width=4.5}
ggplot(cps, aes(x = education_gp, y = wage, fill = education_gp)) + 
    geom_boxplot() + 
    theme_bw() + 
    labs(title = "Boxplot: Wage ~ education group (in years)")+
    guides(fill=FALSE)
```

]

---

# Wages, education and gender

* So, wages are higher for men and higher educated people, well, nothing really new. 

* But, are the return to education the same for women and men? 

* In other words, how the gender gap evolves when considering different levels of education?  

  * Do you have any prior about this?

--

.pull-left[
```{r, echo = FALSE, fig.height=2.8,fig.width=5}
ggplot(cps, aes(x = gender, y = wage, fill = gender)) + 
    geom_boxplot() + 
    facet_wrap(~education_gp, ) +
    theme_bw() + 
    labs(title = "Education group (in years)")+
    guides(fill=FALSE)
```
]

--

.pull-right[
<br/>

* We do observe a gender gap for each education group

* But it's not really clear if/how it varies with education

* Let's test it with a regression!  

]

---

class:inverse

# Task 4 (10 min)

Load the data `CPS1985` from the `AER` package

* Look at the `help` to get the definition of each variable: `?CPS1985`

  * We don't know if people are working part time or full time, does it matter here? 

* Add the`log_wage` variable (log of the `wage`)

* Regress the `log_wage` on `gender` and `education`, save it as `reg1`.
  
  * Interpret each coefficient

* Regress the `log_wage` on `gender`, `education` and their interaction `gender*education`, save it as `reg2`.
  * Do the results confirm our previous guess?

* Add all the other regressors available in our data and run a new regression saved as `reg3`.

---

# Wage, Gender and Education

```{r,echo=F, fig.height=2.75, fig.width=5}
ggplot(cps, aes(x = education, y = log_wage, group = gender, color = gender)) +
    geom_point(alpha = 0.2) + 
    geom_smooth(method = "lm", se = F) +
    theme_bw() +
    ylim(0,4)
```

* We provided evidence that the average wage gap is decreasing with education... 

* ...even after controling for several factors like *experience*, *occupation*, *sector*, ...



---

# Teaser for next chapters 

* You may have noticed that since the beginning we always work with **samples** drawn from the overall population. 

* Each time, imagine we could draw another sample from population. 

  * Would we obtain the same results? 
  
  * In other words, how confident can we be that our estimates (sign, magnitude) are not just driven by hazard. 

* The next chapters will answwer those kind of questions:

  * We'll present the notion of **sampling**
  
  * Understand what **statistical inference** is and how to do it. 
  


---


class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# SEE YOU NEXT WEEK


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

