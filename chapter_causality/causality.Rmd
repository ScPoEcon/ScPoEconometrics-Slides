---
title: "ScPoEconometrics"
subtitle: "Intro To Causality"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

overwrite = FALSE

library(tidyverse)
library(ggplot2)
library(emo)
library(dplyr)
library(png)
library(grid)
library(pander)
```

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Recap from last week

* *Simple Linear Regression Model*: $y_i = b_0 + b_1 x_i + e_i$

* *Ordinary Least Squares* (OLS) estimation: minimize the sum of squared errors

* R command to estimate a linear model: `lm(dependent variable ~ independent variable, data)`


## Today - Introduction to causal inference

* Causality versus correlation

* The Potential Outcome Framework a.k.a. Rubin's Causal Model

* Randomized controlled trials (RCTs)

* Follow up on the empirical application of *class size* and *student performance*

---

# Why do we care about causality?

Many of the *interesting questions* we might want to answer with data are causal

--

- ***Understanding*** the world

  - *Social sciences*: Why do people behave in the way they do?
  - *Health sciences*: Why do people get sick? Which medicine can cure them?

--

- Causal understanding is also of first interest to **policymakers**

  - How to lower unemployment?
  - How to improve student learning?
  - Whether governments should care about the level of public debt?

--

- Note that some questions we might want to answer are not causal 
  - Most *Artificial Intelligence* tasks only care about **prediction**
  - *Example*: predicting whether a photo is of a dog or a cat is vital to how Google Images works, but it doesn't care what *caused* the photo to be of a dog or a cat.

---

# Causality and Economics

- Making causal inference from data can be seen as economists' *comparative advantage* among the social sciences!

- Plenty of fields do statistics. But very few make it standard training for their students to understand causality.

- Economists' endeavour to establish causal relationships is also what makes them useful both in the private (e.g. tech companies) and public sector (e.g. policy advisors). 

--

- Ok, that's enough preaching `r emo::ji("sweat_smile")`


---

# The Concept of Causality

__Causality__: what are we talking about? 

- We say that `X` *causes* `Y`

--

  - if we were to intervene and *change* the value of `X` ***without changing anything else***...
    
--

  - then `Y` would also change ***as a result***.
  
--

- The key point here is the ***without changing anything else***, often referred as the **ceteris paribus (*all else equal*) assumption**.   
(*latin* makes things seem more complicated `r emo::ji("nerd_face")`)

--

- `r emo::ji("warning")` It does **NOT** mean that `X` is the only factor that causes `Y`.

---

# Correlation vs Causation

***Correlation does not equal causation*** has become a ubiquitous mantra, but can you tell why it is true?

--

Some correlations obviously don't imply causation ([e.g. spurious correlation website](https://www.tylervigen.com/spurious-correlations)).

--

```{r, echo = FALSE, out.width = "800px"}
knitr::include_graphics("../img/photos/spurious.png")
```

---

# Correlation vs Causation: Smoking and Lung Cancer

But not all correlations are so easy to rule out

--

***Does smoking cause lung cancer?***

--

.pull-left[
- Today, we know the answer is *YES*! 

- But let's go back in the 1950's

  - We are at the start of a big increase in deaths from lung cancer...
  
  - ... which is happening after a fast growth of cigarette consumption
]

--

.pull-right[
```{r, echo = FALSE, out.width = "400px"}
knitr::include_graphics("../img/photos/Smoking_lung_cancer.png")
```
]

--

- It's very tempting to claim that smoking causes lung cancer based on this graph.

---

# Correlation vs Causation: Smoking and Lung Cancer

At the time many people were still skeptical, including some famous statisticians:

--

.pull-left[
***Macro confouding factors***:  

Other macro factors which can cause cancers also changed between 1900 and 1950:

  - Tarring of roads,
  
  - Inhalation of motor exhausts (leaded gasoline fumes),
  
  - General greater air pollution.
]

--

.pull-right[
***Self selection***: 

Smokers and non-smokers may be different in the first place: 
  
  - __Selection on observable characteristics__: age, education, income, etc.
  
  - __Selection on unobservable characteristics__: genes (the hypothetical confounding genome theory of Fisher [ADD LINK]). 
]

---

# Correlation vs Causation: Smoking and Lung Cancer

- Let's focus on one of these potential confounding characteristics: **age**.


- Based on [Cochran (1968)](https://www.jstor.org/stable/2528036?origin=crossref&seq=11#metadata_info_tab_contents), we will use death rates from lung cancer in Canada, the U.K. and the U.S.

- Add simple details about sample size and years! <= **PIERRE**

Death rates per 1,000 person-years

![:scale 50%](../img/photos/raw_death_rates.png)


- Cigar/pipe smokers are 30% (U.S.) to 75% (Canada) more likely to die from lung cancer than non-smokers.

---

# Correlation vs. causality : smoking and lung cancer #2

* But the age distribution is also very different according to smoking status

Mean age by smoking status

![:scale 50%](../img/photos/age_by_smoking_status.png)

* Because health is likely to deteriorate with age the previous table could be far from causal estimates. 

---

class:inverse

# Correlation vs. causality : smoking and lung cancer #3

Let's consider adjust our statistics taking into account the age distribution of pipe-smokers.

![:scale 40%](../img/photos/ex_cigar_subclassif.png)


This table replicates the 3 age group distribution of pipe-smokers and non-smokers, as well as the death rate of pipe-smokers for each age group in Canada.

**Questions** 

- Compute the average death rate for pipe smokers in Canada from age grouped death rates?

- What would be the average mortality rate be for pipe smokers if they had the same age distribution as the non-smokers?

---


# Correlation vs. causality : smoking and lung cancer #4

Here is the adjusted death rates table found by [(Cochran 1968)](https://www.jstor.org/stable/2528036?origin=crossref&seq=11#metadata_info_tab_contents) using 3 age groups

![:scale 40%](../img/photos/adjusted_death_rates.png)

- Cigars/pipes seem much less dangerous than in the previous table...

- ... but this table still does not allow us to tell about the causal effect of smoking. 

- There are still a lot of **confounding factors** that we are not controling for. 

---

# How Can We Tell?

- Sometimes correlations are just pure *artefacts*: there is no causal relationship between the variables of interest.

- In some other cases, there are both correlation and causality but not of the same **magnitude**, or even the same **direction**.

--

- So how can we make causal inference then? 

- The **Potential Outcomes Framework** will be our guide.



---
layout: false
class: title-slide-section-red, middle

# Causal Inference

---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# The Potential Outcomes Framework

Often called the **Rubin Causal Model** in memory of the statistician **Donald Rubin** who generalised and formalized this model in the 1970's.

--

***Key idea***: Each individual can be exposed to **multiple alternative treatment states**.
  - smoking cigarrettes, smoking cigars or not smoking,
  - growing up in a poor vs a middle class neighborhood vs a rich neighborhood,
  - being in a small or a big class.
  
--

.pull-left[
For practicality, let this treatment variable $D_i$ be a binary variable:

$$
D_i = \begin{cases} 
                    1 \textrm{ if individual $i$ is treated} \\\\ 
                    0 \textrm{ if individual $i$ is not treated} 
      \end{cases}
$$
]

--

.pull-right[

***Treatment group***: all the individuals such that $D_i = 1$.

***Control group***: all the individuals such that $D_i = 0$.
]

---


# The Potential Outcomes Framework

* In this framework, each individual has two ***potential outcomes***:
  
  - $Y_i^1$: *potential outcome in the treatment state* $(D_i = 1)$ for individual $i$,
  
  - $Y_i^0$: *potential outcome in the control state* $(D_i = 0)$ for individual $i$.

--

* From these we can define the ***individual treatment effect***:

$$ \delta_i = Y_i^1 - Y_i^0$$

* $\delta_i$ measures the **causal effect of $D_i$ (treatment)** for individual $i$ on outcome $Y$.

--

* In real life we only observe $Y_i$ which can be written as:

$$Y_i = D_i * Y_i^1 + (1- D_i)*Y_i^0$$

--

* ***Fundamental Problem of Causal Inference***: for any individual $i$, we only observe one of either potential outcomes, and thus we cannot compute $\delta_i$ [(Holland, 1986)](http://people.umass.edu/~stanek/pdffiles/causal-holland.pdf).

---

# The Potential Outcomes Framework

```{r, echo = FALSE, out.width = "700px"}
knitr::include_graphics("../img/photos/fundamental_pb_inference.png")
```


* The potential outcome that is not observed exists in principle, it is called the ***counterfactual outcome***.

  * What your test score would have been if you had been in a big class, knowing that you were in a small one.

--

* Since the treatment effect *cannot* be observed at the individual level, we estimate treatment effects at the population level.

.footnote[
This table is from Morgan and Winship (2015, p.46).
]
---

# Average Treatment Effect (ATE)

Broadest possible average effect: **A**verage **T**reatment **E**ffect (***ATE***)

\begin{align}
ATE &= \mathop{\mathbb{E}}(\delta_i) \\
    &= \mathop{\mathbb{E}}(Y_i^1 - Y_i^0) \\ 
    &= \mathop{\mathbb{E}}(Y_i^1) - \mathop{\mathbb{E}}(Y_i^0)
\end{align}
  
* The ATE simply measures the ***average of individual treatment effects over the whole population***. 
  
  * The $\mathop{\mathbb{E}}(.)$ operator stands for **expectation** or *population mean*.
  
  * The $\mathop{\mathbb{E}}(.)$ operator is linear, in other words, $\mathop{\mathbb{E}}(X+Y) = \mathop{\mathbb{E}}(X) + \mathop{\mathbb{E}}(Y)$ with $X$ and $Y$ being two random variables.

---

# Average Treatment on the Treated (ATT)

Other ***conditional*** average treatment effects may be of interest:

* The **A**verage **T**reatment Effect on the **T**reated (***ATT***)

\begin{align}
 ATT &= \mathop{\mathbb{E}}(\delta_i | D_i = 1) \\
     &= \mathop{\mathbb{E}}(Y_i^1 - Y_i^0 | D_i = 1) \\
     &= \mathop{\mathbb{E}}(Y_i^1 | D_i = 1) - \mathop{\mathbb{E}}(Y_i^0 | D_i = 1)
\end{align}

* The ATT measures the ***average treatment effect conditional on being in the treatment group***.

    * The $\mathop{\mathbb{E}}(.|D = x)$ operator stands for **conditional expectation**. It refers to the expectation over a subcategory of the entire population, namely people who satisfy the condition $D = x$.
  * The $\mathop{\mathbb{E}}(.|D = x)$ operator is also linear.

---

# Average Treatment on the Untreated (ATU)

Other ***conditional*** average treatment effects may be of interest:

* The **A**verage **T**reatment Effect on the **U**ntreated (***ATU***)

\begin{align}
 ATU &= \mathop{\mathbb{E}}(\delta_i | D_i = 0) \\
     &= \mathop{\mathbb{E}}(Y_i^1 - Y_i^0 | D_i = 0) \\
     &= \mathop{\mathbb{E}}(Y_i^1 | D_i = 0) - \mathop{\mathbb{E}}(Y_i^0 | D_i = 0)
\end{align}

* The ATU measures the ***average treatment effect conditional on being in the control group***.

--

* In the majority of cases, ATE $\neq$ ATT $\neq$ ATU!

---

# The Problem of Causal Inference

* We have the same **missing data problem** for computing the ATE, ATT or ATU as we did for $\delta_i$. Either $Y_i^1$ or $Y_i^0$ is missing for each $i$.

--

* From the data, we can compute the **S**imple **D**ifference in mean **O**utcomes (***SDO***) for both groups:

$$ SDO = \mathop{\mathbb{E}}(Y_i^1|D_i=1) - \mathop{\mathbb{E}}(Y_i^0|D_i=0) $$ 

* For example, it would consist in taking the difference between:
  * Death rates for smokers and non-smokers,
  * GDP growth rates for democratic and non democratic countries,
  * Unemployment rates for countries with and without a minimum wage.

* Most of the time, such a difference **will fail to capture the causal effect** of the treatment.

* Notice that this kind raw comparison is often done by journalists, politicians, badly trained scientists (but not you now! `r emo::ji("wink")`).

---

# The Problem of Causal Inference

Let's rewrite the SDO to make the individual treatment effect $(\delta_i)$ appear in the equation. 

\begin{align}
  \mathop{\mathbb{E}}(Y_i^1|D_i=1) - \mathop{\mathbb{E}}(Y_i^0|D_i=0) &= \mathop{\mathbb{E}}(Y_i^0 + \delta_i | D_i = 1) - \mathop{\mathbb{E}}(Y_i^0 | D_i = 0)
\end{align}

For simplicity, suppose **treatment effect is constant** across people: $\forall i, \delta_i = \delta$.

Then,

\begin{align}
  \mathop{\mathbb{E}}(Y_i^1|D_i=1) - \mathop{\mathbb{E}}(Y_i^0|D_i=0) &= \delta + \mathop{\mathbb{E}}(Y_i^0 | D_i = 1) - \mathop{\mathbb{E}}(Y_i^0 | D_i = 0)
\end{align}

And because $ATE = \mathop{\mathbb{E}}(\delta_i) \underbrace{=}_\text{constant treatment effect} \mathop{\mathbb{E}}(\delta) = \delta$, we get: 


\begin{align}
  SDO &= \mathop{\mathbb{E}}(Y_i^1|D_i=1) - \mathop{\mathbb{E}}(Y_i^0|D_i=0) \\
  &= ATE + \underbrace{\mathop{\mathbb{E}}(Y_i^0 | D_i = 1) - \mathop{\mathbb{E}}(Y_i^0 | D_i = 0)}_\text{Selection bias}
\end{align}

---

class:inverse

# Task 2 (10 minutes)

Let's compute these various quantities and biases with some toy data (i.e. data we generated ourselves).

```{r, echo = FALSE, eval = FALSE}
set.seed(1)
D_i = sample(c(0,1), replace = T, size = 1000)
Y0_i = sample(c(1:10), replace = T, size = 1000) + D_i
delta_i = sample(c(1:3), replace = T, size = 1000) + 0.2*D_i
cor(Y0_i, D_i)
cor(delta_i, D_i)
Y1_i = Y0_i + delta_i

set.seed(3)
D_i_random = sample(c(0,1), replace = T, size = 1000)
cor(Y0_i, D_i_random)
cor(delta_i, D_i_random)

toy_data = data.frame(D_i, Y0_i, Y1_i, D_i_random)
fwrite(toy_data, "../../../../../Dropbox/ScPoEconometrics/pierre_gustave/toy_data.csv")
# toy_data = read.csv("https://www.dropbox.com/s/a4u67nmu9gnl94y/toy_data.csv?dl=1")
```

1. Load the data [here](https://www.dropbox.com/s/a4u67nmu9gnl94y/toy_data.csv?dl=1). Notice that `Di_random` is a treatment status we would have under random assignment.

1. Create the following variables: $Y_i$ and $\delta_i$. Recall that $Y_i = D_i * Y_i^1 + (1 - D_i) Y_i^0$ and $\delta_i = Y_i^1 - Y_i^0$. 

1. Compute the ***ATE*** and the ***SDO***. (Use base `R`.)  Is there is any *bias*? Is it large in magnitude?

1. Using `D_i_random`, compute the ***SDO under randomization***. Remember that you need to recompute $Y_i$ because the assignment is not the same anymore.   
If you got it right, the bias should be very close to 0. Why is it not exactly 0? 

1. *To do at home*:  Compute the value of the ***selection bias*** and the ***heterogenous treat effect bias*** and check that we have $$SDO = ATE + \textrm{selection bias} + \textrm{heter. trt. effect bias}$$


---

# Randomization solves the pb of causal inference ! 

* Let's imagine you can **randomly** assign people to each treatment group. That is what **randomized experiments** are. 

* Then the treatment status would be **independant** of the potential outcomes.

* In particular, there is no reason for $\mathop{\mathbb{E}}(Y_i^0 | D_i = 1)$ to be different from $\mathop{\mathbb{E}}(Y_i^0 | D_i = 0)$

  * So the **selection bias is equal to O** 
  
* In the same way, so there is no reason for $\delta_i$ to be different among treated than among controls. 

  * So ATT will equal ATU, implying the **Heterogenous treatment effect bias to be 0** 

---

# Randomization solves the pb of causal inference ! 

Thanks to randomization we then have : 

$$ \mathop{\mathbb{E}}(Y_i^1|D_i=1) - \mathop{\mathbb{E}}(Y_i^0|D_i=0) = ATE$$ 

It means we can directly estimate the ATE from the data! 

---

layout: false
class: title-slide-section-red, middle

# Randomized Experiments

---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Randomized experiments

- Often called **R**andomized **C**ontrolled **T**rials (RCT).

---

# Back to class size and students' achievements

Last week we regressed class size on average student math and reading scores.

$$\textrm{math score}_i = b_0 + b_1 \textrm{class size}_i + e_i$$
We briefly discuss why $b_1^{OLS}$ could only establish an ***association*** and not a ***causal relationship***.

--

* **Student sorting**: There is selection into schools with different sized classes. Suppose parents have a prior that smaller classes are better, they will try to get their kids into those schools.

--

* **Teacher sorting**: Teachers may sort in schools with smaller classes because itâ€™s easier to teach a small rather than a large class, and if there is competition for those places then higher quality teachers will have an advantage.

--

* **Location effect**: Large classes may be more common in wealthier and bigger cities, while small classes may be more likely in poorer rural areas.

--

An RCT would take care of all these biases!

---

# The Project STAR Experiment

Tennessee **S**tudent/**T**eacher **A**chievement **R**atio Experiment (see [Krueger (1999)](http://piketty.pse.ens.fr/files/Krueger1999.pdf))

* Funded by Tennesse legislature for a total cost of approx. $12 million.

* The experiment started in the 1985-1986 school year and lasted four years.

--

* 11,600 students and their teachers where **randomly assigned** to one of the following 3 groups from kindergarten through third grade:

  1. ***Small class***: 13-17 students per teacher,
  
  2. ***Regular class***: 22-25 students,
  
  3. ***Regular/aide class***: 22-25 students with a full-time teacher's *aide*.

--

* Randomization occurred within schools.

* Students' math and reading skills were tested around March each year.

--

* There was a problem of ***non-random attrition*** but we will ignore it.

---

class:inverse

# Task 3 (10 minutes)

1. Load the *STAR* data from [here]() and assign it to an object called `star_df`.

1. Read the help for `AER::STAR` to understand what the variables correspond to. (Note: the data has been *reshaped* so don't mind the "k", "1", etc. in the variable names in the help.)

1. What's the unit of observation? Which variable contains: (i) the (random) class assignment, (ii) the student's class grade, (iii) the outcomes of interest?

1. How many observations are there? Why so many?

1. Why are there so many `NA`s? What do they correspond to?

1. Keep only cases with no `NA`s with the following code:  
`star_df <- star_df[complete.cases(star_long),]`

1. Let's check how well the randomization was done by doing ***balancing checks***.  
Compute the average percentage of girls, african americans, and free lunch qualifiers by grade *and* treatment class.  
*Hint*: The following computes the percentage of girls (without the relevant `dplyr` verbs)
`share_female = mean(gender == "female") * 100`.


```{r, echo = FALSE}
# data("STAR", package = "AER")
# star <- STAR %>%
#     pivot_longer(
#         cols = -c(gender, ethnicity, birth),
#         names_to = c(".value", "grade"),
#         names_pattern = "(.+)(k|1|2|3)$")
# write.csv2(star, file = "star_data.csv", row.names = F)
star_df = read.csv(file = "https://www.dropbox.com/s/bf1fog8yasw3wjj/star_data.csv?dl=1")
star_df <- star_df[complete.cases(star_df),]
```

---

# The Project STAR Experiment

We just saw that in an RCT the ATE is obtained by computing the differences in outcomes between the treatment and control groups.

Let's only focus on:

- One treatment group: **small classes**,

- One control group: **regular classes**,

- One grade: **kindergarten** (k).

--

```{r, echo = FALSE}
diff_table = data.frame(
    grade = rep(c("1","2","3","k"), each = 2),
    test = rep(c("math","read"), times = 4),
    star_long  %>%
        pivot_longer(cols = c("math","read"), names_to = "test", values_to = "score") %>%
        filter(star == "regular") %>%
        group_by(grade, test) %>%
        summarise(mean_regular = round(mean(score), 3)) %>%
        ungroup() %>%
        select(mean_regular),
    star_long %>%
        pivot_longer(cols = c("math","read"), names_to = "test", values_to = "score") %>%
        filter(star == "small") %>% group_by(grade, test) %>%
        summarise(mean_small = round(mean(score), 3)) %>%
        ungroup() %>%
        select(mean_small),
    star_long %>%
        pivot_longer(cols = c("math","read"), names_to = "test", values_to = "score") %>%
        filter(star == "regular+aide") %>%
        group_by(grade, test) %>%
        summarise(mean_regular_aide = round(mean(score), 3)) %>%
        ungroup() %>%
        select(mean_regular_aide)) %>%
    mutate(
        diff_small_regular = round(mean_small - mean_regular, 3),
        diff_regular_aide_regular = round(mean_regular_aide - mean_regular, 3)
    ) %>%
    arrange(factor(grade, levels = c("k","1","2","3")))
```

grade | test | mean regular | mean small | ATE
--------|---------|---------|---------|---------
k | math | `r round(diff_table[1,3], 2)` | `r round(diff_table[1,4], 2)` | `r round(diff_table[1,4] - diff_table[1,3], 2)`
k | read | `r round(diff_table[2,3], 2)` | `r round(diff_table[2,4], 2)` | `r round(diff_table[2,4] - diff_table[2,3], 2)`

What's the interpretation for these ATEs?

--

That's nice but can't we put this in regression form?

---

# RCT in Regression Form

$$ Y_i = D_i Y_i^1 + (1 - D_i) Y_i^0 $$

--

Rewriting this equation, we get:

$$\begin{align} Y_i &=Y_i^0 +D_i (Y_i^1 - Y_i^0) \\ &= Y_i^0 +D_i \delta_i \end{align}$$

--

Assuming $\delta_i = \delta, \forall i$,

$$Y_i = Y_i^0 + D_i \delta$$

--

Adding $\mathbb{E}[Y_i^0] - \mathbb{E}[Y_i^0] = 0$ to the right-hand side:

$$\begin{align} Y_i &= \mathbb{E}[Y_i^0] + D_i \delta + Y_i^0 - \mathbb{E}[Y_i^0] \\ &= b_0 + \delta D_i + e_i \end{align}$$
where $b_0 = \mathbb{E}[Y_i^0]$ and $e_i = Y_i^0 - \mathbb{E}[Y_i^0]$

---

# The Project STAR Experiment: Regression

The last equation looks exactly like the simple regression model we saw last week!

Let's therefore estimate the ATE of small class size on math scores using a regression.

--

.pull-left[
We want to estimate the following model: $mathscore_i = b_0 + \delta D_i + e_i$, with

$$
D_i = \begin{cases} 
                    1 \textrm{  if small class = 1}  \\\\ 
                    0 \textrm{  if small class = 0} 
      \end{cases}
$$

```{r, echo = TRUE}
star_df_k_small <- star_df %>%
  filter(
    star %in% c("regular", "small") &
      grade == "k") %>% 
  mutate(small = (star == "small"))
```
]

--

.pull-right[
```{r, echo = TRUE, eval = TRUE}
lm(math ~ small, star_df_k_small)
```

```{r, echo = TRUE, eval = TRUE}
mean(star_df_k_small[
    star_df_k_small$small == 1, "math"]) - 
  mean(star_df_k_small[
    star_df_k_small$small == 0, "math"])

mean(star_df_k_small[
    star_df_k_small$small == 0, "math"])
```
]

---

# The Project STAR Experiment: Regression

From the estimation output we get the following:

$\begin{align} \mathbb{E}[\textrm{math score} | D_i = 0]&= \mathbb{E}[b_0 + \delta D_i + e_i | D_i = 0] \\ &= b_0 + \delta \mathbb{E}[D_i| D_i = 0] \\ &= b_0 \end{align}$

--

$\begin{align} \mathbb{E}[\textrm{math score} | D_i = 1]&= \mathbb{E}[b_0 + \delta D_i + e_i | D_i = 1] \\ &= b_0 + \delta \mathbb{E}[D_i| D_i = 1] \\ &= b_0 + \delta \end{align}$

--

$\begin{align} ATE &= \mathbb{E}[\textrm{math score} | D_i = 1] - \mathbb{E}[\textrm{math score} | D_i = 0] \\ &= b_0 + \delta - b_0 \\ &= \delta \end{align}$

--

We knew this already but we now understand why this is true `r emo::ji("v")`

---

class:inverse

# Task 4 (10 minutes)

1. Filter the dataset to only keep first graders and the small class and regular class groups.

1. Compute the average math score for both groups, and the difference between the two. (Use base `R`.)

1. Create a dummy variable `treatment` equal to `TRUE` if student is in treatment group (i.e. small class size) and `FALSE` if in control group (i.e. regular class size). See slide 33 for how to create a dummy variable.

1. Regress the treatment dummy variable on math score. Are the results in line with the previous question?

1. How do you interpret these coefficients?

---

# RCTs Shortcomings 

Although randomized experiments make causal inference much more convincing it also has several shortcomings 

* RCT are often **infeasible**

  * RCTs are **costly**
  * RCTs may face some **ethical issues** : some *treatment* just cannot be given to people
  * RCTs take time and we may be **time constrained** 

--

* **Interpretating** the results

  * **External validity issue**: How much the results from a given RCT's can be generalized to other context (countries, populations,...?
  
  * Identifying the mechanisms that are at stake may be difficult

---


# What comes next? 

* So if we cannot rely on RCTs to make our life easy, it means we have to find a way to make causal inference from **observational data** (as opposed to experimental data) 

* It brings us back to models 

  - In causal inference, the *model* is our idea of what the process that *generated the data* is.

  - We have to make some assumptions about what this is!

* If we are confident that **selection occurs on the observables**, i.e. variables that we have in our data : we will use **Multiple Regression**
  * That's next course !
  
* If we think that some confounding factors are unobserved : we will have to rely on more *fancy* techniques, such as **RDD** or **Diff-in-Diff**
  * That's session 10 and 11 ! 


---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

