---
title: "ScPoEconometrics"
subtitle: "Intro To Causality"
author: "F. Oswald, G. Kennedy and P. Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

overwrite = FALSE

library(ggplot2)
library(emo)
library(dplyr)
library(png)
library(grid)
library(pander)
```

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---


# Recap from from past Lectures and plan for this session

**Recap of past session**

* Simple Linear Regression model 

* R knowledge : run an OLS regression and interpret the output


**Plan for today** : *Introduction to causal inference*

* Causality versus correlation

* The Potential Outcome Model

* Randomized experiments

* Follow up : the case of **class size** and **students performance** 

---

# Why do we care about causality?

Many of the interesting questions we might want to answer with data are causal<sup>1</sup>

--

- Understanding the world
    - Social sciences : Why people behave in the way they do? 
    - Health sciences : XXX

--

- Public policy perspective
    - How to reduce unemployment?
    - XXX 

--

Note that some questions we might want to answer are non causal 
  - Most *Artificial Intelligence* tasks, for example : "how can we predict whether a photo is of a dog or a cat" is vital to how Google Images works, but it doesn't care what *caused* the photo to be of a dog or a cat.
    
.footnote[
[1]: This set of slides follows closely the great material by [Nick Huntington Klein](https://nickch-k.github.io/introcausality/)
]

---

# Why causality 

- This is (kind of) economists' comparative advantage!

- Plenty of fields do statistics. But very few make it standard training for their students to understand causality.

- This understanding of causality makes economists very useful! *This* is one big reason why tech companies have whole economics departments in them

---

# Causality : Definition

- We say that `X` *causes* `Y` if...

--

  - we were to intervene and *change* the value of `X` without changing anything else...
    
--

  - then `Y` would also change as a result

--

This is the so-called **ceteris paribus assumption**

- But it does **NOT** mean that...

  - `X` is the only cause of `Y`, other factors could *cause* `Y`
    
  - XXX
    
---

# DAG 


---

# Correlation vs Causation

* That's becomes almost a trueism: *Correlation is not equal Causation*.

* Well, we're here to figure out conditions when it **is**!

* Econometrics is about stating the conditions under which we can claim that a relationship is causal.

---

# Correlation vs. causality : some obvious cases

Check the [Spurious correlation website](https://www.tylervigen.com/spurious-correlations)

![:scale 70%](../img/photos/spurious.png)

- Who would believe that sociologist can help into space lauching? `r emo::ji("smile")`

- Some case are more difficult to tell
---

# Correlation vs. causality : a more debated case #1

**Does smoking cause lung cancer?** 

- Today, we know the answer and it is YES! 

- But let's go back in the 1950's

  - We are at the start of a big increase in deaths from lung cancer...
  
  - ... which is happening after a fast growth of cigarette consumption

--

.pull-right[
![:scale 60%](../img/photos/Smoking_lung_cancer.png)
]

---

# Correlation vs. causality : a more debated case #1

.pull-left[
![:scale 80%](../img/photos/Smoking_lung_cancer.png)
]

.right-large[

Skepticism about the causal impact of smoking :

- Other macro factors have also changed between 1900 and 1950
  - Tarring of roads
  - Inhalation of motor exhausts (leaded gasoline fumes)
  - General greater air pollution.

- Smokers and non smokers are not the same 
  - Selection on observables (age, education, income, ...)
  - Selection on unobservables : the hypothetical confounding genome theory of Fisher. 

]

---

# Correlation vs. causality : Lung cancer and smoking #2

- Let's focus on death rates among smokers and non smokers in 3 countries [(Cochran 1968)](https://www.jstor.org/stable/2528036?origin=crossref&seq=11#metadata_info_tab_contents)

Death rates per 1,000 person-years

![:scale 40%](../img/photos/raw_death_rates.png)

--

- What seems to be the most dangerous? 

--

- In the same time we observe smokers to be older on average

![:scale 40%](../img/photos/age_by_smoking_status.png)


---

class:inverse

# **Task**

Let's consider this table.

![:scale 40%](../img/photos/ex_cigar_subclassif.png)



- What is the average death rate for pipe smokers?

- What would the average mortality rate be for pipe smokers if they had the same age distribution as the non-smokers?

---


# Correlation vs. causality : Lung cancer and smoking #3

Here is the adjusted death rates using 3 age groups

![:scale 40%](../img/photos/adjusted_death_rates.png)

- Cigars and pipes do not seem to be as dangerous as in the first table.

- It does not mean that cigars are not dangerous, but it shows that the first table provide no evidence about the causal impact of smoking on lung cancer 

---

# So How Can We Tell?

- As just shown, there are plenty of *correlations* that aren't *causal*

- So if we have a correlation, how can we tell if it *is*?

- For this we're going to have to think hard about *causal inference*. That is, inferring causality from data.

---
layout: false
class: title-slide-section-red, middle

# Causal Inference

---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# The Problem of Causal Inference

.pull-left[
- Let's try to think about whether some `X` causes `Y`

- That is, if we manipulated `X`, then `Y` would change as a result

- For simplicity, let's assume that `X` is either 1 or 0, like "got a medical treatment" or "didn't"

]

--

.pull-right[

## The Problem of Causal Inference

- Now, how can we know *what would happen* if we manipulated `X`?

- Let's consider just one person - Angela. We could just check what Angela's `Y` is when we make `X=0`, and then check what Angela's `Y` is again when we make `X=1`.

- Are those two `Y`s different? If so, `X` causes `Y`!

- Do that same process for everyone in your sample and you know in general what the effect of `X` on `Y` is.

]

---

# The Problem of Causal Inference


- You may have spotted the problem!

- Just like you can't be in two places at once, Angela can't exist both with `X=0` and with `X=1`. She either got that medical treatment or she didn't.

- Let's say she did. So for Angela, `X=1` and, let's say, her outcome is `Y=10`.

- The other one, what `Y` *would have been* if we made `X=0`, is **missing**. 

- We don't know what it is! Could also be `Y=10`. Could be `Y=9`. Could be `Y=1000`!


---


# The Problem of Causal Inference

- Well, why don't we just take someone who actually DOES have `X=0` and compare their `Y`?

- Because there are lots of reasons their `Y` could be different BESIDES `X`.

- They're not Angela! A character flaw to be sure. `r emo::ji("grinning")`

- So if we find someone, Gareth, with `X=0` and they have `Y=9`, is that because `X` increases `Y`, or is that just because Angela and Gareth would have had different `Y`s anyway?

---


# The Problem of Causal Inference

- The main goal we have in doing causal inference is in making *as good a guess as possible* as to what that `Y` *would have been* if `X` had been different.

- That "would have been" is called a *counterfactual* - counter to the fact of what actually happened.

- In doing so, we want to think about two people/firms/countries that are basically *exactly the same* except that one has `X=0` and one has `X=1`.

- If we could, we'd pick someone *identical* to Angela, the only difference being that they had `X=0` and not `X=1`.

---

# The Potential Outcomes Model

also called Rubin Causal Model 

Definition 1


---


# The Potential Outcomes Model

Definition 2

---

# The Potential Outcomes Model

ATE

---
layout: false
class: title-slide-section-red, middle

# Experiments

---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Experiments

.pull-left[
- A common way to do this in many fields is an *experiment*.

- If you can *randomly assign* `X`, then you know that the people with `X=0` are, on average, exactly the same as the people with `X=1`.

- So that's an easy comparison!

]

--

.pull-right[


- When we're working with people/firms/countries, running experiments is often infeasible, impossible, or unethical.

- So we have to think hard about a *model* of what the world looks like.

- So that we can use our model to figure out what the *counterfactual* would be.

]


---

# Back to class size and students' achievements

In past session we regress some average student standardized score on class size. 

$$\textrm{avg_math_score}_i = b_0 + b_1 \textrm{class_size}_i + e_i$$
Do you think $b_1$ is measuring the causal impact of class size on students' math score? If not, why ? 

--

* Students sorting : There is selection into schools with different sized classes. Suppose parents have a prior that smaller classes are better - they will try to get their kids into those schools.

--

* Teachers sorting : teachers could sort towards schools with smaller classes because itâ€™s easier to teach a small rather than a large class, and if there is competition for those places and higher quality teachers will have an advantage.

--

If you were asked to think of an experiment that captures the influence of class size on student performance, what would it be? 

---

# The STAR Experiment (1/5)
(Tennessee Student/Teacher Achievement Ratio Experiment)

* Starting in 1985-1986 and lasting for four years, young pupils starting Kindergarden and their teachers where randomly allocated to to several possible groups : 

1. Small classes with 13-17 students
2. Regular classes with 22-25 students
3. Regular classes with 22-25 students but with an additional full-time teaching aide.

* Randomization at the school level (i.e. students and teachers of a given school are randomized across class types)

* Let's take their data and replicate some of the experimental findings!

---

# Implementing STAR (2/5)

Load the data from the `AER` package
```{r}
data("STAR", package = "AER")
library(data.table)
x = as.data.table(STAR)
```

```{r, echo = FALSE}
x[, ID := 1:nrow(x)]  # add student ID

# `melt` a data.table means to dissolve it and reassamble for some ID variables
mx = melt.data.table(x, 
                     id = c("ID","gender","ethnicity","birth"), 
                     measure.vars = patterns("star*","read*","math*", "schoolid*",
                                             "degree*","experience*","tethnicity*","lunch*"), 
                     value.name = c("classtype","read","math","schoolid","degree",
                                    "experience","tethniticy","lunch"),
                     variable.name = "grade")

levels(mx$grade) <- c("stark","star1","star2","star3")  # reassign levels to grade factor
#mx[,1:8]  # show first 8 cols

mx = mx[complete.cases(mx)]
#mx[ID==2]  # here is pupil number 2
```

---

# Implementing STAR (3/5)

Standardizing grades

.pull-left[

```{r echo = FALSE,fig.height=5.5, results="hide"}
#mx[,range(read)]
#mx[,range(math)]

setkey(mx, classtype)  # key mx by class type
ecdfs = mx[classtype != "small",        # subset data.table to this
    list(readcdf = list(ecdf(read)),    # create cols readcdf and mathcdf
         mathcdf = list(ecdf(math))
         ),
         by = grade]    # by grade

# let's look at those cdf!
om = par("mar")
par(mfcol=c(4,2),mar = c(2,om[2],2.5,om[4]))
ecdfs[,.SD[,plot(mathcdf[[1]],main = paste("math ecdf grade",.BY))],by = grade]
ecdfs[,.SD[,plot(readcdf[[1]],main = paste("read ecdf grade",.BY))],by = grade]

par(mfcol=c(1,1),mar = om)

# <p style="text-align: center;"> Empirical Cumulative Distribution Function of scores </p>

```

]

--

.pull-right[

* Those graphs are **Empirical Cumulative Distribution Functions** : it gives the ranking - between 0 and 1 - associated to a given score. 

* What is the median of the "read score" in 3rd grade?

* Why don't we use the raw scores directly? 

]

---

# Implementing STAR (4/5)

Comparing the achievments between each group : **Graphical analysis**

.pull-left[

```{r echo = FALSE, fig.height=5, fig.width=6.5}
setkey(ecdfs, grade)  # key ecdfs according to `grade`
setkey(mx,grade)

z = mx[ , list(ID,perc_read = ecdfs[(.BY),readcdf][[1]](read),
               perc_math = ecdfs[(.BY),mathcdf][[1]](math)),
        by=grade]   # stick `grade` into `ecdfs` as `.BY`

z[,score := rowMeans(.SD)*100, .SDcols = c("perc_read","perc_math")]  # take average of scores
# and multiply by 100, so it's comparable to Krueger

# merge back into main data
mxz = merge(mx,z,by = c("grade","ID"))

# make a plot
ggplot(data = mxz, mapping = aes(x = score,color=classtype)) + geom_density() + facet_wrap(~grade) + theme_bw()
```
]

--

.pull-right[

* We first construct a global ranking based on both read and math score's ranking of student $i$ :

$$\textrm{Final score}_i = 100*\frac{Rank^{read}_{i}+Rank^{read}_{i}}{2}$$

* We compare the density function between each experimental group for the kindergarden and grade 1 to 3. 

* Which group seems to be the most successful? 

]

---

# Implementing STAR (5/5)

Comparing the achievments between each group : **Regressions**

The model we estimate : $Y_{i} = b_0 + b_1\textrm{SMALL}_{i} + b_2 \textrm{REG&A}_{i} + e_i$

.pull-left[

```{r echo = FALSE}
# create Krueger's dummy variables
mxz =  as_tibble(mxz) %>%
    mutate(small = classtype == "small",
           rega  = classtype == "regular+aide",
           girl  = gender == "female",
           freelunch = lunch == "free")

# reproduce columns 1 in Kruger
m1 = mxz %>% 
    group_by(grade) %>%
    do(model = lm(score ~ small + rega, data = .))

h = list()

h <- huxtable::huxreg("K" = subset(m1,grade == "stark")$model[[1]],
                      "1" = subset(m1,grade == "star1")$model[[1]],
                      "2" = subset(m1,grade == "star2")$model[[1]],
                      "3" = subset(m1,grade == "star3")$model[[1]],
                      statistics = c(N = "nobs", R2 = "r.squared"),
                      number_format = 2, 
                      stars = NULL, 
                      bold_signif = 0.01
                      ) %>% 
  huxtable::theme_article() %>%
  huxtable::set_caption("Estimates for each grade") %>%
  huxtable::set_font_size(12) %>%
  huxtable::set_top_border(8, 1:5, 2)
h
```
]

--

.pull-right[

* Based on these results, would you advise to reduce class size or provide additional teaching aide? 

* What is the value of $b_1$ for grade3? Interpret this value.

* What are the limits of such an experiment? 

]
---

# Models

* We cannot always run RCTs 

- In causal inference, the *model* is our idea of what we think the process is that *generated the data*.

- We have to make some assumptions about what this is!

- We put together what we know about the world with assumptions and end up with our model.

- The model can then tell us what kinds of things could give us wrong results so we can fix them and get the right counterfactual.

---

# Models

- Wouldn't it be nice to not have to make assumptions?

- Yeah, but it's impossible to skip!

- We're trying to predict something tat hasn't happened - a counterfactual.

- This is literally impossible to do if you don't have some model of how the data is generated.

- You can't even predict the sun will rise tomorrow without a model!

- If you think you can, you're just don't realize the model you're using - that's dangerous!

---

# An Example: Randomization

- Let's generate some data.
- Let's say that getting `X` causes `Y` to increase by 100.
- And let's run a randomized experiment of who actually gets X and estimate the size of the effect (should be close to `100`!)

.pull-left[
```{r, echo=TRUE, eval=TRUE}
true_effect <- 100
df <- tibble(Y.without.X = rnorm(1000),
             X=sample(c(0,1),1000,replace=T)) %>%
  mutate(Y.with.X = Y.without.X + true_effect) %>%
  #Now assign who actually gets X
  mutate(Observed.Y = ifelse(X==1,Y.with.X,Y.without.X))
head(df,4)
```
]

.pull-right[
* Now, estimate the group means and simply difference them:
```{r}
#And see what effect our experiment suggests X has on Y
df %>% group_by(X) %>% 
  summarize(Y = mean(Observed.Y)) %>%
  pull(Y) %>% diff
```
]

* That's pretty close to the true effect there!

---

# An Example: No Randomization

- Suppose now we can *not* randomize the `X`.

- Instead there is some kind of rule that decides about `X`.

.pull-left[
```{r, echo=TRUE, eval=TRUE}
df <- tibble(Z = runif(10000)) %>% 
  mutate(Y.without.X = rnorm(10000)+100*Z, 
         Y.with.X = Y.without.X + 100) %>%
  #Now assign who actually gets X
  mutate(X = Z > .9,
         Observed.Y = ifelse(X==1,Y.with.X,Y.without.X))
head(df)
```
]

--

.pull-right[

```{r}
df %>% group_by(X) %>% 
  summarize(Y = mean(Observed.Y)) %>%
  pull(Y) %>% diff
```

* That's not the correct effect size!

* But if we properly model the assignment process and compare apples to apples?

```{r}
df %>% filter(abs(Z-.9)<.01) %>% 
  group_by(X) %>% 
  summarize(Y = mean(Observed.Y)) %>%
  pull(Y) %>% diff
```
]

---

# Building the Counterfactual

* In that last example our first attempt failed because we had not taken selection into account.

* It turns out that `Z` influences `Y.without.X`, which at the same time will be part of our estimator for the true effect size.

* But `Z` also determines who gets treatment! All the ones with `Z > 0.9`!

* So, people with `Z > 0.9` get treated, but **also** have a relatively high `Y.without.X`. 

* You can see that there is a depenence between the assignment and the effect size.


---

class: title-slide-final, middle

# THANKS

To XXX

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

